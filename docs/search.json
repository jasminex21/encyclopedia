[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Encyclopedia",
    "section": "",
    "text": "Hi, I’m Jasmine! Welcome!\nI love learning - so, this is my personal encyclopedia - a repository of knowledge, if you will - where I store writeups of topics I am interested in, ranging from psychology to computer science to paleontology.\nThis is a more tech-y and accessible continuation of a trend I started way back in grade school, during which I kept a thick notebook covered in a floral pattern, filled with handwritten writeups of miscellaneous topics (I still have it!). I used to keep that notebook secret because I thought it would make me look too nerdy, but I’m at a point in life where I’m more than happy to embrace and share my nerdiness. So, welcome to my encyclopedia!\n\nVisit the Ideabank!\n\n\n\nWriteups\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDate\n\n\nTags\n\n\n\n\n\n\nLinguistic Intergroup Bias\n\n\nOct 29, 2024\n\n\nPsychology\n\n\n\n\nSpeed Dating Questions to Build Connections\n\n\nJan 5, 2025\n\n\nNotes\n\n\n\n\nThe Diary of a CEO - Vanessa Van Edwards\n\n\nJan 5, 2025\n\n\nNotes, Personal favorite\n\n\n\n\nMusic Preference & Personality\n\n\nJan 8, 2025\n\n\nPsychology, Personal favorite\n\n\n\n\nN-gram Language Models\n\n\nJan 16, 2025\n\n\n\n\n\n\n\nMachine Learning with Graphs\n\n\nJul 14, 2025\n\n\nComputer Science\n\n\n\n\nIntroduction to NetworkX and Pytorch Geometric\n\n\n \n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "topics/computer_science/neural_networks/03_intro_to_networkx_pytorch.html",
    "href": "topics/computer_science/neural_networks/03_intro_to_networkx_pytorch.html",
    "title": "Introduction to NetworkX and Pytorch Geometric",
    "section": "",
    "text": "NetworkX is one of the most frequently used Python packages to create, manipulate, and mine graphs.\n\n\n\n# import the NetworkX package\nimport networkx as nx\n\n\n\n\nNetworkX provides several classes to store different types of graphs, such as directed and undirected graph. It also provides classes to create multigraphs (both directed and undirected).\nFor more information, please refer to NetworkX graph types.\n\n# create an undirected graph G\nG = nx.Graph()\nprint(G.is_directed())\n\n# create a directed graph H\nH = nx.DiGraph()\nprint(H.is_directed())\n\n# add graph level attribute\nG.graph[\"Name\"] = \"Bar\"\nprint(G.graph)\n\nFalse\nTrue\n{'Name': 'Bar'}\n\n\n\n\n\nNodes (with attributes) can be easily added to NetworkX graphs.\n\n# add one node with node level attributes\nG.add_node(0, feature=0, label=0)\n\n# get attributes of the node 0\nnode_0_attr = G.nodes[0]\nprint(\"Node 0 has the attributes {}\".format(node_0_attr))\n\nNode 0 has the attributes {'feature': 0, 'label': 0}\n\n\n\n# add multiple nodes with attributes\nG.add_nodes_from([\n  (1, {\"feature\": 1, \"label\": 1}),\n  (2, {\"feature\": 2, \"label\": 2})\n])\n\n# loop through all the nodes\n# set data=True will return node attributes\nfor node in G.nodes(data=True):\n  print(node)\n\n# get number of nodes\nnum_nodes = G.number_of_nodes()\nprint(\"G has {} nodes\".format(num_nodes))\n\n(0, {'feature': 0, 'label': 0})\n(1, {'feature': 1, 'label': 1})\n(2, {'feature': 2, 'label': 2})\nG has 3 nodes\n\n\n\n\n\nSimilar to nodes, edges (with attributes) can also be easily added to NetworkX graphs.\n\n# add one edge with edge weight 0.5\nG.add_edge(0, 1, weight=0.5)\n\n# get attributes of the edge (0, 1)\nedge_0_1_attr = G.edges[(0, 1)]\nprint(\"Edge (0, 1) has the attributes {}\".format(edge_0_1_attr))\n\nEdge (0, 1) has the attributes {'weight': 0.5}\n\n\n\n# add multiple edges with edge weights\nG.add_edges_from([\n  (1, 2, {\"weight\": 0.3}),\n  (2, 0, {\"weight\": 0.1})\n])\n\n# loop through all the edges\n# here there is no data=True, so only the edge will be returned\nfor edge in G.edges():\n  print(edge)\n\n# get number of edges\nnum_edges = G.number_of_edges()\nprint(\"G has {} edges\".format(num_edges))\n\n(0, 1)\n(0, 2)\n(1, 2)\nG has 3 edges\n\n\n\n\n\n\n# draw the graph\nnx.draw(G, with_labels = True)\n\n\n\n\n\n\n\n\n\n\n\n\nnode_id = 1\n\n# degree of node 1\nprint(\"Node {} has degree {}\".format(node_id, G.degree[node_id]))\n\n# get neighbor of node 1\nfor neighbor in G.neighbors(node_id):\n  print(\"Node {} has neighbor {}\".format(node_id, neighbor))\n\nNode 1 has degree 2\nNode 1 has neighbor 0\nNode 1 has neighbor 2\n\n\n\n\n\nNetworkX also provides plenty of useful methods to study graphs.\nHere is an example to get PageRank of nodes (we will talk about PageRank in one of the future lectures).\n\nnum_nodes = 4\n# create a new path like graph and change it to a directed graph\nG = nx.DiGraph(nx.path_graph(num_nodes))\nnx.draw(G, with_labels = True)\n\n# get the PageRank\npr = nx.pagerank(G, alpha=0.8)\npr\n\n{0: 0.17857162031103999,\n 1: 0.32142837968896,\n 2: 0.32142837968896,\n 3: 0.17857162031103999}\n\n\n\n\n\n\n\n\n\n\n\n\nYou can explore more NetworkX functions through its documentation.",
    "crumbs": [
      "Topics",
      "Computer Science",
      "Neural Networks",
      "Introduction to NetworkX and Pytorch Geometric"
    ]
  },
  {
    "objectID": "topics/computer_science/neural_networks/03_intro_to_networkx_pytorch.html#networkx-tutorial",
    "href": "topics/computer_science/neural_networks/03_intro_to_networkx_pytorch.html#networkx-tutorial",
    "title": "Introduction to NetworkX and Pytorch Geometric",
    "section": "",
    "text": "NetworkX is one of the most frequently used Python packages to create, manipulate, and mine graphs.\n\n\n\n# import the NetworkX package\nimport networkx as nx\n\n\n\n\nNetworkX provides several classes to store different types of graphs, such as directed and undirected graph. It also provides classes to create multigraphs (both directed and undirected).\nFor more information, please refer to NetworkX graph types.\n\n# create an undirected graph G\nG = nx.Graph()\nprint(G.is_directed())\n\n# create a directed graph H\nH = nx.DiGraph()\nprint(H.is_directed())\n\n# add graph level attribute\nG.graph[\"Name\"] = \"Bar\"\nprint(G.graph)\n\nFalse\nTrue\n{'Name': 'Bar'}\n\n\n\n\n\nNodes (with attributes) can be easily added to NetworkX graphs.\n\n# add one node with node level attributes\nG.add_node(0, feature=0, label=0)\n\n# get attributes of the node 0\nnode_0_attr = G.nodes[0]\nprint(\"Node 0 has the attributes {}\".format(node_0_attr))\n\nNode 0 has the attributes {'feature': 0, 'label': 0}\n\n\n\n# add multiple nodes with attributes\nG.add_nodes_from([\n  (1, {\"feature\": 1, \"label\": 1}),\n  (2, {\"feature\": 2, \"label\": 2})\n])\n\n# loop through all the nodes\n# set data=True will return node attributes\nfor node in G.nodes(data=True):\n  print(node)\n\n# get number of nodes\nnum_nodes = G.number_of_nodes()\nprint(\"G has {} nodes\".format(num_nodes))\n\n(0, {'feature': 0, 'label': 0})\n(1, {'feature': 1, 'label': 1})\n(2, {'feature': 2, 'label': 2})\nG has 3 nodes\n\n\n\n\n\nSimilar to nodes, edges (with attributes) can also be easily added to NetworkX graphs.\n\n# add one edge with edge weight 0.5\nG.add_edge(0, 1, weight=0.5)\n\n# get attributes of the edge (0, 1)\nedge_0_1_attr = G.edges[(0, 1)]\nprint(\"Edge (0, 1) has the attributes {}\".format(edge_0_1_attr))\n\nEdge (0, 1) has the attributes {'weight': 0.5}\n\n\n\n# add multiple edges with edge weights\nG.add_edges_from([\n  (1, 2, {\"weight\": 0.3}),\n  (2, 0, {\"weight\": 0.1})\n])\n\n# loop through all the edges\n# here there is no data=True, so only the edge will be returned\nfor edge in G.edges():\n  print(edge)\n\n# get number of edges\nnum_edges = G.number_of_edges()\nprint(\"G has {} edges\".format(num_edges))\n\n(0, 1)\n(0, 2)\n(1, 2)\nG has 3 edges\n\n\n\n\n\n\n# draw the graph\nnx.draw(G, with_labels = True)\n\n\n\n\n\n\n\n\n\n\n\n\nnode_id = 1\n\n# degree of node 1\nprint(\"Node {} has degree {}\".format(node_id, G.degree[node_id]))\n\n# get neighbor of node 1\nfor neighbor in G.neighbors(node_id):\n  print(\"Node {} has neighbor {}\".format(node_id, neighbor))\n\nNode 1 has degree 2\nNode 1 has neighbor 0\nNode 1 has neighbor 2\n\n\n\n\n\nNetworkX also provides plenty of useful methods to study graphs.\nHere is an example to get PageRank of nodes (we will talk about PageRank in one of the future lectures).\n\nnum_nodes = 4\n# create a new path like graph and change it to a directed graph\nG = nx.DiGraph(nx.path_graph(num_nodes))\nnx.draw(G, with_labels = True)\n\n# get the PageRank\npr = nx.pagerank(G, alpha=0.8)\npr\n\n{0: 0.17857162031103999,\n 1: 0.32142837968896,\n 2: 0.32142837968896,\n 3: 0.17857162031103999}\n\n\n\n\n\n\n\n\n\n\n\n\nYou can explore more NetworkX functions through its documentation.",
    "crumbs": [
      "Topics",
      "Computer Science",
      "Neural Networks",
      "Introduction to NetworkX and Pytorch Geometric"
    ]
  },
  {
    "objectID": "topics/computer_science/neural_networks/03_intro_to_networkx_pytorch.html#pytorch-geometric-tutorial",
    "href": "topics/computer_science/neural_networks/03_intro_to_networkx_pytorch.html#pytorch-geometric-tutorial",
    "title": "Introduction to NetworkX and Pytorch Geometric",
    "section": "PyTorch Geometric Tutorial",
    "text": "PyTorch Geometric Tutorial\nPyTorch Geometric (PyG) is an extension library for PyTorch. It provides useful primitives to develop Graph Deep Learning models, including various graph neural network layers and a large number of benchmark datasets.\nThis tutorial is adapted from https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=ci-LpZWhRJoI by Matthias Fey\n\nimport torch\nprint(\"PyTorch has version {}\".format(torch.__version__))\n\nPyTorch has version 2.0.1+cu117\n\n\n\nSetup\nThe installation of PyG on Colab can be a little bit tricky. Execute the cell below – in case of issues, more information can be found on the PyG’s installation page.\n\n# Install torch geometric\n# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n# !pip install -q torch-geometric\n\n\n\nVisualization\n\n# helper function for visualization.\n%matplotlib inline\nimport torch\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# visualization function for NX graph or PyTorch tensor\ndef visualize(h, color, epoch=None, loss=None):\n    plt.figure(figsize=(7,7))\n    plt.xticks([])\n    plt.yticks([])\n\n    if torch.is_tensor(h):\n        h = h.detach().cpu().numpy()\n        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n        if epoch is not None and loss is not None:\n            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n    else:\n        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n                         node_color=color, cmap=\"Set2\")\n    plt.show()\n\n\n\nIntroduction\nRecently, deep learning on graphs has emerged to one of the hottest research fields in the deep learning community.\nHere, Graph Neural Networks (GNNs) aim to generalize classical deep learning concepts to irregular structured data (in contrast to images or texts) and to enable neural networks to reason about objects and their relations.\nThis tutorial will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the PyTorch Geometric (PyG) library.\nPyTorch Geometric is an extension library to the popular deep learning framework PyTorch, and consists of various methods and utilities to ease the implementation of Graph Neural Networks.\nFollowing Kipf et al. (2017), let’s dive into the world of GNNs by looking at a simple graph-structured example, the well-known Zachary’s karate club network. This graph describes a social network of 34 members of a karate club and documents links between members who interacted outside the club. Here, we are interested in detecting communities that arise from the member’s interaction.\n\n\nDataset\nPyTorch Geometric provides an easy access to the dataset via the torch_geometric.datasets subpackage:\n\nfrom torch_geometric.datasets import KarateClub\n\ndataset = KarateClub()\nprint(f'Dataset: {dataset}:')\nprint('======================')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')\n\nDataset: KarateClub():\n======================\nNumber of graphs: 1\nNumber of features: 34\nNumber of classes: 4\n\n\nAfter initializing the KarateClub dataset, we first can inspect some of its properties.\nFor example, we can see that this dataset holds exactly one graph, and that each node in this dataset is assigned a 34-dimensional feature vector (which uniquely describes the members of the karate club).\nFurthermore, the graph holds exactly 4 classes, which represent the community each node belongs to.\nLet’s now look at the underlying graph in more detail:\n\ndata = dataset[0]  # get the first graph object.\n\nprint(data)\nprint('==============================================================')\n\n# gather some statistics about the graph.\nprint(f'Number of nodes: {data.num_nodes}')\nprint(f'Number of edges: {data.num_edges}')\nprint(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\nprint(f'Number of training nodes: {data.train_mask.sum()}')\nprint(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\nprint(f'Contains isolated nodes: {data.has_isolated_nodes()}')\nprint(f'Contains self-loops: {data.has_self_loops()}')\nprint(f'Is undirected: {data.is_undirected()}')\n\nData(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34])\n==============================================================\nNumber of nodes: 34\nNumber of edges: 156\nAverage node degree: 4.59\nNumber of training nodes: 4\nTraining node label rate: 0.12\nContains isolated nodes: False\nContains self-loops: False\nIs undirected: True\n\n\n\n\nData\nEach graph in PyTorch Geometric is represented by a single Data object, which holds all the information to describe its graph representation.\nWe can print the data object anytime via print(data) to receive a short summary about its attributes and their shapes:\nData(edge_index=[2, 156], x=[34, 34], y=[34], train_mask=[34])\nWe can see that this data object holds 4 attributes:\n\nThe edge_index property holds the information about the graph connectivity, i.e., a tuple of source and destination node indices for each edge.\n\nPyG further refers to (2) node features as x (each of the 34 nodes is assigned a 34-dim feature vector), and to (3) node labels as y (each node is assigned to exactly one class).\n\nThere also exists an additional attribute called train_mask, which describes for which nodes we already know their community assigments.\n\nIn total, we are only aware of the ground-truth labels of 4 nodes (one for each community), and the task is to infer the community assignment for the remaining nodes.\nThe data object also provides some utility functions to infer some basic properties of the underlying graph.\nFor example, we can easily infer whether there exists isolated nodes in the graph (i.e. there exists no edge to any node), whether the graph contains self-loops (i.e., \\((v, v) \\in \\mathcal{E}\\)), or whether the graph is undirected (i.e., for each edge \\((v, w) \\in \\mathcal{E}\\) there also exists the edge \\((w, v) \\in \\mathcal{E}\\)).\n\nfrom IPython.display import Javascript  # restrict height of output cell.\ndisplay(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n\nedge_index = data.edge_index\nprint(edge_index.t())\n\n\n\n\ntensor([[ 0,  1],\n        [ 0,  2],\n        [ 0,  3],\n        [ 0,  4],\n        [ 0,  5],\n        [ 0,  6],\n        [ 0,  7],\n        [ 0,  8],\n        [ 0, 10],\n        [ 0, 11],\n        [ 0, 12],\n        [ 0, 13],\n        [ 0, 17],\n        [ 0, 19],\n        [ 0, 21],\n        [ 0, 31],\n        [ 1,  0],\n        [ 1,  2],\n        [ 1,  3],\n        [ 1,  7],\n        [ 1, 13],\n        [ 1, 17],\n        [ 1, 19],\n        [ 1, 21],\n        [ 1, 30],\n        [ 2,  0],\n        [ 2,  1],\n        [ 2,  3],\n        [ 2,  7],\n        [ 2,  8],\n        [ 2,  9],\n        [ 2, 13],\n        [ 2, 27],\n        [ 2, 28],\n        [ 2, 32],\n        [ 3,  0],\n        [ 3,  1],\n        [ 3,  2],\n        [ 3,  7],\n        [ 3, 12],\n        [ 3, 13],\n        [ 4,  0],\n        [ 4,  6],\n        [ 4, 10],\n        [ 5,  0],\n        [ 5,  6],\n        [ 5, 10],\n        [ 5, 16],\n        [ 6,  0],\n        [ 6,  4],\n        [ 6,  5],\n        [ 6, 16],\n        [ 7,  0],\n        [ 7,  1],\n        [ 7,  2],\n        [ 7,  3],\n        [ 8,  0],\n        [ 8,  2],\n        [ 8, 30],\n        [ 8, 32],\n        [ 8, 33],\n        [ 9,  2],\n        [ 9, 33],\n        [10,  0],\n        [10,  4],\n        [10,  5],\n        [11,  0],\n        [12,  0],\n        [12,  3],\n        [13,  0],\n        [13,  1],\n        [13,  2],\n        [13,  3],\n        [13, 33],\n        [14, 32],\n        [14, 33],\n        [15, 32],\n        [15, 33],\n        [16,  5],\n        [16,  6],\n        [17,  0],\n        [17,  1],\n        [18, 32],\n        [18, 33],\n        [19,  0],\n        [19,  1],\n        [19, 33],\n        [20, 32],\n        [20, 33],\n        [21,  0],\n        [21,  1],\n        [22, 32],\n        [22, 33],\n        [23, 25],\n        [23, 27],\n        [23, 29],\n        [23, 32],\n        [23, 33],\n        [24, 25],\n        [24, 27],\n        [24, 31],\n        [25, 23],\n        [25, 24],\n        [25, 31],\n        [26, 29],\n        [26, 33],\n        [27,  2],\n        [27, 23],\n        [27, 24],\n        [27, 33],\n        [28,  2],\n        [28, 31],\n        [28, 33],\n        [29, 23],\n        [29, 26],\n        [29, 32],\n        [29, 33],\n        [30,  1],\n        [30,  8],\n        [30, 32],\n        [30, 33],\n        [31,  0],\n        [31, 24],\n        [31, 25],\n        [31, 28],\n        [31, 32],\n        [31, 33],\n        [32,  2],\n        [32,  8],\n        [32, 14],\n        [32, 15],\n        [32, 18],\n        [32, 20],\n        [32, 22],\n        [32, 23],\n        [32, 29],\n        [32, 30],\n        [32, 31],\n        [32, 33],\n        [33,  8],\n        [33,  9],\n        [33, 13],\n        [33, 14],\n        [33, 15],\n        [33, 18],\n        [33, 19],\n        [33, 20],\n        [33, 22],\n        [33, 23],\n        [33, 26],\n        [33, 27],\n        [33, 28],\n        [33, 29],\n        [33, 30],\n        [33, 31],\n        [33, 32]])\n\n\n\n\nEdge Index\nBy printing edge_index, we can further understand how PyG represents graph connectivity internally.\nWe can see that for each edge, edge_index holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge.\nThis representation is known as the COO format (coordinate format) commonly used for representing sparse matrices.\nInstead of holding the adjacency information in a dense representation \\(\\mathbf{A} \\in \\{ 0, 1 \\}^{|\\mathcal{V}| \\times |\\mathcal{V}|}\\), PyG represents graphs sparsely, which refers to only holding the coordinates/values for which entries in \\(\\mathbf{A}\\) are non-zero.\nWe can further visualize the graph by converting it to the networkx library format, which implements, in addition to graph manipulation functionalities, powerful tools for visualization:\n\nfrom torch_geometric.utils import to_networkx\n\nG = to_networkx(data, to_undirected=True)\nvisualize(G, color=data.y)\n\n\n\n\n\n\n\n\n\n\nImplementing Graph Neural Networks\nAfter learning about PyG’s data handling, it’s time to implement our first Graph Neural Network!\nFor this, we will use one of the most simple GNN operators, the GCN layer (Kipf et al. (2017)).\nPyG implements this layer via GCNConv, which can be executed by passing in the node feature representation x and the COO graph connectivity representation edge_index.\nWith this, we are ready to create our first Graph Neural Network by defining our network architecture in a torch.nn.Module class:\n\nimport torch\nfrom torch.nn import Linear\nfrom torch_geometric.nn import GCNConv\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super(GCN, self).__init__()\n        torch.manual_seed(12345)\n        self.conv1 = GCNConv(dataset.num_features, 4)\n        self.conv2 = GCNConv(4, 4)\n        self.conv3 = GCNConv(4, 2)\n        self.classifier = Linear(2, dataset.num_classes)\n\n    def forward(self, x, edge_index):\n        h = self.conv1(x, edge_index)\n        h = h.tanh()\n        h = self.conv2(h, edge_index)\n        h = h.tanh()\n        h = self.conv3(h, edge_index)\n        h = h.tanh()  # final GNN embedding space.\n\n        # apply a final (linear) classifier.\n        out = self.classifier(h)\n\n        return out, h\n\nmodel = GCN()\nprint(model)\n\nGCN(\n  (conv1): GCNConv(34, 4)\n  (conv2): GCNConv(4, 4)\n  (conv3): GCNConv(4, 2)\n  (classifier): Linear(in_features=2, out_features=4, bias=True)\n)\n\n\nHere, we first initialize all of our building blocks in __init__ and define the computation flow of our network in forward.\nWe first define and stack three graph convolution layers, which corresponds to aggregating 3-hop neighborhood information around each node (all nodes up to 3 “hops” away).\nIn addition, the GCNConv layers reduce the node feature dimensionality to \\(2\\), i.e., \\(34 \\rightarrow 4 \\rightarrow 4 \\rightarrow 2\\). Each GCNConv layer is enhanced by a tanh non-linearity.\nAfter that, we apply a single linear transformation (torch.nn.Linear) that acts as a classifier to map our nodes to 1 out of the 4 classes/communities.\nWe return both the output of the final classifier as well as the final node embeddings produced by our GNN.\nWe proceed to initialize our final model via GCN(), and printing our model produces a summary of all its used sub-modules.\n\nmodel = GCN()\n\n_, h = model(data.x, data.edge_index)\nprint(f'Embedding shape: {list(h.shape)}')\n\nvisualize(h, color=data.y)\n\nEmbedding shape: [34, 2]\n\n\n\n\n\n\n\n\n\nRemarkably, even before training the weights of our model, the model produces an embedding of nodes that closely resembles the community-structure of the graph.\nNodes of the same color (community) are already closely clustered together in the embedding space, although the weights of our model are initialized completely at random and we have not yet performed any training so far!\nThis leads to the conclusion that GNNs introduce a strong inductive bias, leading to similar embeddings for nodes that are close to each other in the input graph.\n\n\nTraining on the Karate Club Network\nBut can we do better? Let’s look at an example on how to train our network parameters based on the knowledge of the community assignments of 4 nodes in the graph (one for each community):\nSince everything in our model is differentiable and parameterized, we can add some labels, train the model and observe how the embeddings react.\nHere, we make use of a semi-supervised or transductive learning procedure: We simply train against one node per class, but are allowed to make use of the complete input graph data.\nTraining our model is very similar to any other PyTorch model. In addition to defining our network architecture, we define a loss critertion (here, CrossEntropyLoss) and initialize a stochastic gradient optimizer (here, Adam).\nAfter that, we perform multiple rounds of optimization, where each round consists of a forward and backward pass to compute the gradients of our model parameters w.r.t. to the loss derived from the forward pass.\nIf you are not new to PyTorch, this scheme should appear familar to you.\nOtherwise, the PyTorch docs provide a good introduction on how to train a neural network in PyTorch.\nNote that our semi-supervised learning scenario is achieved by the following line:\nloss = criterion(out[data.train_mask], data.y[data.train_mask])\nWhile we compute node embeddings for all of our nodes, we only make use of the training nodes for computing the loss.\nHere, this is implemented by filtering the output of the classifier out and ground-truth labels data.y to only contain the nodes in the train_mask.\nLet us now start training and see how our node embeddings evolve over time (best experienced by explicitely running the code):\n\nimport time\nfrom IPython.display import Javascript  # restrict height of output cell.\ndisplay(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 430})'''))\n\nmodel = GCN()\ncriterion = torch.nn.CrossEntropyLoss()  # define loss criterion.\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # define optimizer.\n\ndef train(data):\n    optimizer.zero_grad()  # clear gradients.\n    out, h = model(data.x, data.edge_index)  # perform a single forward pass.\n    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # compute the loss solely based on the training nodes.\n    loss.backward()  # derive gradients.\n    optimizer.step()  # update parameters based on gradients.\n    return loss, h\n\nfor epoch in range(400):\n    loss, h = train(data)\n    # visualize the node embeddings every 10 epochs\n    if epoch % 20 == 0:\n        visualize(h, color=data.y, epoch=epoch, loss=loss)\n        time.sleep(0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs one can see, our 3-layer GCN model manages to linearly separate the communities and classifying most of the nodes correctly.\nFurthermore, we did this all with a few lines of code, thanks to the PyTorch Geometric library which helped us out with data handling and GNN implementations.\n\n\nDocumentation\nYou can explore more PyG functions through its documentation.",
    "crumbs": [
      "Topics",
      "Computer Science",
      "Neural Networks",
      "Introduction to NetworkX and Pytorch Geometric"
    ]
  },
  {
    "objectID": "topics/notes/01_the_science_of_people.html#how-to-be-more-likable",
    "href": "topics/notes/01_the_science_of_people.html#how-to-be-more-likable",
    "title": "The Diary of a CEO - Vanessa Van Edwards",
    "section": "How to be more likable",
    "text": "How to be more likable\nResearch has shown that a huge part of why certain people are deemed “popular” or more “likable” is simply because they themselves like more people. For example, simply saying hi to people you know whilst passing by helps them perceive you as more likable, because you like them. Make people feel so liked that they like you back. To facilitate this, three magic phrases for likeability (use as much as desired):\n\n“I was just thinking of you!” / “&lt;X&gt;, and it reminded me of you!”\n\nCan be used as a transition into a “how are you?”\nLet this occur naturally - e.g., if you watch a documentary and it reminds you of someone, text that person and let them know.\n\n“You’re always so &lt;X&gt;”\n\nGive the other party a positive label. This is especially effective because it fights signal amplification bias (people don’t always know what you think of them!).\n\n“Last time we talked, you mentioned &lt;X&gt;…”\n\nCan be used like “Hey, the last time we talked, you mentioned that big project you were really excited about, how’s that going?”\nPeople are honored when you remember stuff about them.",
    "crumbs": [
      "Topics",
      "Notes",
      "The Diary of a CEO - Vanessa Van Edwards"
    ]
  },
  {
    "objectID": "topics/notes/01_the_science_of_people.html#how-to-be-charismatic",
    "href": "topics/notes/01_the_science_of_people.html#how-to-be-charismatic",
    "title": "The Diary of a CEO - Vanessa Van Edwards",
    "section": "How to be CHARISMATIC",
    "text": "How to be CHARISMATIC\nTo be charismatic, you have to be highly competent and highly warm, and more importantly, you need to signal that you are highly competent and highly warm. People cannot trust you if you do not send enough competency or warmth cues.\nA preface: gestures are massive in charisma. When speaking to someone, make sure your hands are always available to gesture. The significance of gestures is that they help indicate honesty and competency, and they also contribute to vocal intonations and variety (if you sit on your hands during a conversation or interview, the amount of your vocal intonations and variety will automatically decrease).\n\nPower cues of competence\nCompetence = power, reliability, and capability. How to channel competence through cues:\n\nThe steeple gesture (i.e. making a triangle with your hands).\n\nFound to be the highest-ranked gesture that leaders use.\nShows you are not hiding anything, yet you are relaxed and poised.\nDo NOT drum your hands / fingers.\n\nMaximize the distance between your earlobe and shoulder.\n\nThis distance is correlated with confidence; high shoulders indicates anxiety (and also prevents you from using full vocal power). People do not like anxious people. Relax your shoulders down, keep your earlobes out. This makes you both look more confident and feel more confident.\n\nMake eye contact at the end of your sentences, and ideally when the other person is making an important point.\n\nMaintaining 100% eye contact in a conversation feels invasive and awkward; 60-70% is more ideal.\nPeople like when you process and gather information or memories from all around, but then look back at them at the end of the sentence to drill a point.\n\nUsing and noticing the lower lid flex.\n\nWhen you are trying to see something far away, you harden your lower lid to try and see it better (basically squinting). When you are really trying to focus on something, they flex their lower lid - people like this, because it shows that you are focused on what they are saying.\nNote that this is not an inherently positive cue - it is a cue of focus, and is also important to notice when people are doing it (in addition to you yourself using it). So, if you are, say, presenting something, and someone flexes their lower lid, you can follow up with them and ask “does this make sense?” and try to gather more information and clarify concerns. This allows you to prevent skepticism and confusion.\n\nUsing downward vocal inflections, and preventing accidental question inflections.\n\nVocal cues are crucial because they tell people how you feel about them, and how you feel about yourself.\nThe question inflection is when your tone goes up at the end of a sentence, like you are asking a question. Do NOT use the question inflection when you are not asking a question, because it makes people doubt and scrutinize you. In fact, people tend to use it when telling a lie.\nImportantly, do NOT ask your name when you introduce yourself. You want to be perceived as confident when making a first impression.\nGo DOWN at the end of sentences - this makes people think that you really belieave in what you are saying, and they will take you seriously.\n\n\n\n\nPower cues of warmth\nWarmth = trust, likeability, and friendliness. Pay close attention to these if you’ve been told you’re hard to talk to, intimidating, or cold, or if people don’t tend to open up to you. How to channel warmth through cues:\n\nA slow triple nod.\n\nThis draws people to tell you more - research has shown that the slow triple nod makes people talk longer.\nDo NOT bobble-head, and do NOT nod too quickly, because nodding too quickly implies impatience, and is basically telling the other person to stfu.\n\nHead tilt.\n\nTilting your head makes you look like you are listening and that you care.\nResearch has shown that if you tilt your head when delivering bad news, you are more likable - people take the news better.\nThis is good to use if you nod too much.\n\nAn authentic smile.\n\nAn authentic, genuine smile should reach all the way up to your eyes and hit your cheek muscles. Genuine happiness is contagious.\nFake smiles arhave the same effect as not smiling at all.\n\nLean forward, both as a speaker and a listener.\n\nIf someone is leaning towards you, it makes you feel closer to them.\nYou can lean in as a speaker if you want to emphasize an important point - lean slightly towards the other person to use it as a bold or a highlight. BUT, don’t lean all the time, because highlighting an entire page of a book means nothing.\nYou can lean in as a listener when the other person says something of interest, to show that you are engaged.\nToo much of a lean implies that you are subservient and submissive to the other person (it looks like a bow), and people want equal relationships.\n\nNonverbal bridging - physically bridge the distance between yourself and the other person.\n\nIn impactful conversations, people try to bridge the distance between themselves and the other speaker via bridges like leaning in, hand gestures, nods, drinks, and when close enough, light physical touches on the arm or shoulder. The warmest people are bridging all the time. You are reaching into the other person’s intimate zone, but not staying there.\nYou can even touch without touching - you can hover above the arm / shoulder and then go back.\nClosing bridges is a very warm action - this is why serving or giving people things is so warm.",
    "crumbs": [
      "Topics",
      "Notes",
      "The Diary of a CEO - Vanessa Van Edwards"
    ]
  },
  {
    "objectID": "topics/notes/01_the_science_of_people.html#questions-to-level-up-with-people",
    "href": "topics/notes/01_the_science_of_people.html#questions-to-level-up-with-people",
    "title": "The Diary of a CEO - Vanessa Van Edwards",
    "section": "Questions to level up with people",
    "text": "Questions to level up with people\n\n“Are you working on anything exciting recently / these days?”\n\nStop asking boring questions like “how are you?” or “what do you do?” - this puts people on autopilot.\nAsking someone “what do you do?” can actually be perceived as rude if that person is not defined by their work.\nThis is permission connection - if someone is defined by what they do, they will tell you about what they do. If they are not defined by what they do, they’ll tell you something better.\nThe responses to this question also gives you something to bring up when you next see the person - you can start the conversation with “hey, last time we spoke, you mentioned you were working on &lt;X&gt; - how’s that been going?”\n\n“What’s your biggest goal right now?”\n\nThis is a great question to ask, especially around the new year (Janurary - March).\nGenerally, when you ask this question, people will either 1) shut you down, or 2) talk about their goals. Anyone who shuts you down, saying they “don’t believe in goals” is probably not your person.\nThis is another question that is great to follow up on.\n\n“What book, movie, or TV character is most like you, and why?”\n\nThis is a deeper question that helps you figure out the other person’s narrative, and the response to this question is so important, because how someone relates to characters (their values or personality) is how they see themselves.\n(For future reference, my answer to this question would be Riley from Inside Out)",
    "crumbs": [
      "Topics",
      "Notes",
      "The Diary of a CEO - Vanessa Van Edwards"
    ]
  },
  {
    "objectID": "topics/notes/01_the_science_of_people.html#how-to-improve-your-dating-life",
    "href": "topics/notes/01_the_science_of_people.html#how-to-improve-your-dating-life",
    "title": "The Diary of a CEO - Vanessa Van Edwards",
    "section": "How to improve your dating life",
    "text": "How to improve your dating life\n\nSocial media presence\nYour social media presence should activate the correct neural networks for the right partner and create “allergies” in others (you don’t want to appeal to everyone, because you don’t want to waste your time with the wrong people).\n\nE.g., do NOT overwhelm your Instagram with photos at bars and at parties if your intention is to settle down and have a family with someone.\nE.g., if you want to attract a hard worker, post photos of yourself at conferences and working hard yourself.\n\nYour main profile picture should signal warmth and competence.\n\n\nBody language\nUse your body language to signal that you are confident. Maximize the distance between your earlobe and shoulder, and the distance between your arm and your torso should fluctuate.\n\nIn a good conversation, the distance between the arm and torso fluctuates a lot. People who are anxious are contracted; their arms are stuck to their torso at all times and they have minimal hand gestures. Releasing that space makes you appear more confident and less anxious, and aids with vocal variety.\n\nDo NOT accidentally use vocal fry. People who are contracted also may tend to use vocal fry, which occurs when you are out of breath, causing the vocal chords to rattle. If you hear yourself using it, speak louder.\n\n\nMen and women perceive body language differently - in women, far more brain regions were activated than in men when looking at pictures of body language. Women take in a much broader pictures of body language, while men tend to be more cue-focused. Women tend to globalize cues and over-generalize.\n\n\nHow to get approached\nResearch has shown that people who get approached the most [in clubs] gave out the biggest signals of availability - these people did not necessarily have to be the most attractive. How to signal you are available:\n\nOpen body with no blocking your torso.\n\nNothing should be in front of your torso - do NOT cross your arms, clutch your phone / computer, etc. Keep your torso open and angled out to the room.\nYour feet should also be angled toward the biggest part of the room, signaling you are open. Not parallel feet.\n\nSmall, darting glances to everyone in the room (who you want to approach you).\n\nYou actually need to glance (quickly) at a person ~8 times to get them to approach - a higher number than you probably would’ve expected.\nThese glances should be quick; things like side glances, a side glance with a smile, a flip of the hair and a look over.\n\nGenerally gesturing in the person of interest’s direction.\n\nWhen talking to someone else, gesture in the person of interest’s line of gesturing, almost signalling for them to come over. If that person is at all interested, they are likely to come over.\n\n\nPay close attention to the responses to your gestures. If the person meets your glance, that’s good. If the person turns away from you or turns their feet away from you, they are probably not very receptive to your advances.\nGenerally, people are lonely and really want to meet their person and make friends.",
    "crumbs": [
      "Topics",
      "Notes",
      "The Diary of a CEO - Vanessa Van Edwards"
    ]
  },
  {
    "objectID": "topics/notes/01_the_science_of_people.html#a-framework-for-how-to-make-friends-as-adults",
    "href": "topics/notes/01_the_science_of_people.html#a-framework-for-how-to-make-friends-as-adults",
    "title": "The Diary of a CEO - Vanessa Van Edwards",
    "section": "A framework for how to make friends as adults",
    "text": "A framework for how to make friends as adults\nApproach friendship like dating - you are looking for your friend soulmates. Go on friendship dates, different every time.\n\nTake your friend to places that might make them a bit “allergic,” because it allows you to see their response. Test out values that you appreciate in friends.",
    "crumbs": [
      "Topics",
      "Notes",
      "The Diary of a CEO - Vanessa Van Edwards"
    ]
  },
  {
    "objectID": "topics/notes/01_the_science_of_people.html#how-to-spot-a-liar",
    "href": "topics/notes/01_the_science_of_people.html#how-to-spot-a-liar",
    "title": "The Diary of a CEO - Vanessa Van Edwards",
    "section": "How to spot a liar",
    "text": "How to spot a liar\nThe average person is very bad at spotting a liar, so do not overestimate your ability to do so. Give people the benefit of the doubt. But, that being said, there are cues that repeatedly show up in people who are lying:\n\nVerbal cues:\n\nSudden use of the question inflection.\n\nIf you hear that, especially in a statement, number, timeline, or boundary, it’s a sign for you to dig deeper to see if you hear it again.\n\nA sudden drop in volume.\n\nWhen people are anxious or nervous, we lose breath and our volume drops.\nIf you hear this, it’s a signal for you to dig deeper.\n\n\nNonverbal cues:\n\nIncongruencies where the verbal does not match the body.\n\nThe biggest examples of this is when someone is saying yes but shakes their head no, or when they say no and nod their head yes. People don’t often realize they are shaking their head no whilst saying yes - it is actively difficult to think about what you are saying and what you are doing separately. (Note, though, that there are cultural exceptions in India, Bulgaria, and Pakistan - they nod differently)\n\nMismatched facial expressions, typically disgust.\n\nDisgust is typically denoted by people crinkling their nose and sometimes flashing their upper teeth.\nLiars often feel “dirty” with themselves for lying, and so they’ll show disgust when lying.\nLiars may sometimes sniff when lying as well - it is a disgust activator.\nYou can also use this cue as a speaker - if you see someone making a disgusted facial expression in response to something you’re saying, take it as a sign to go back on it.",
    "crumbs": [
      "Topics",
      "Notes",
      "The Diary of a CEO - Vanessa Van Edwards"
    ]
  },
  {
    "objectID": "topics/notes/01_the_science_of_people.html#how-to-start-a-conversation",
    "href": "topics/notes/01_the_science_of_people.html#how-to-start-a-conversation",
    "title": "The Diary of a CEO - Vanessa Van Edwards",
    "section": "How to start a conversation",
    "text": "How to start a conversation\n\nDon’t overthink your opener.\n\nYour opener can be something as simple as, “hey, I’m Jasmine, nice to meet you.”\nOpeners should actually be basic so that it signals to the other person that you are safe. Do not overcomplicate things.\n\nAfter introducing yourself and obtaining the basic information, ask a version of the excitement question (“have you done anything exciting recently?”) based on the circumstances.\n\nOn Mondays, you can ask, “Do anything exciting this past weekend?”\nOn Fridays, you can ask, “Any exciting plans for this weekend?”",
    "crumbs": [
      "Topics",
      "Notes",
      "The Diary of a CEO - Vanessa Van Edwards"
    ]
  },
  {
    "objectID": "topics/notes/01_the_science_of_people.html#how-to-stop-being-awkward",
    "href": "topics/notes/01_the_science_of_people.html#how-to-stop-being-awkward",
    "title": "The Diary of a CEO - Vanessa Van Edwards",
    "section": "How to stop being awkward",
    "text": "How to stop being awkward\n\nStop competing on stages where you can’t be your best; think about your social strengths, and create spaces where you can exercise them.\nBreak autopilot - ask better questions as to not trigger autopilot. Stop asking basic, boring questions like “how are you?” and “what do you do?” - ask the excitement question or some version of it.\nFind out where you fall on the competence and warmth scales, and address what must be addressed.",
    "crumbs": [
      "Topics",
      "Notes",
      "The Diary of a CEO - Vanessa Van Edwards"
    ]
  },
  {
    "objectID": "topics/psychology/01_intergroup_bias.html",
    "href": "topics/psychology/01_intergroup_bias.html",
    "title": "Linguistic Intergroup Bias",
    "section": "",
    "text": "What is Intergroup Bias?\nIntergroup bias refers to the systematic human tendency to exhibit favoritism towards one’s in-group over the out-group. This favoritism, or bias, may manifest in behaviour (e.g. discrimination), attitude (e.g. prejudice), and/or cognition (e.g. stereotyping), and may take the form of in-group favoritism and/or out-group derogation.\nA key motivator of intergroup bias is Social Identity Theory (Tajfel 1979), which posits that humans derive a portion of their self-identity from their membership in social groups. Humans automatically categorize social groups into “us” (the in-group) and “them” (the out-group) based on attributes such as race, gender, familial relation, or sports team allegiance, and then adopt the behaviours, norms, and values of the group. After identifying with a group, members compare their group to others, and exhibit positive distinctiveness - the process in which members of the in-group are inclined to make their group seem positively distinct from the out-group. As in, not only are the two groups different, the in-group also attempts to make this difference favorable.\nSocial categorization is strong. In fact, Tajfel (1970) demonstrated that mere categorization, even if trivial or nonsensical, is sufficient for in-group favoritism and out-group discrimination. In the study, it was found that British schoolboys categorized based on their preference between abstract artists Paul Klee and Wassily Kandinsky exhibited favoritism towards fellow schoolboys with the same preference, despite the insignificance of the social categorization. The participants were asked to assign points to members of both groups, and it was seen that they tended to do so in such a way that the difference between the two groups was maximized, even if this meant that the in-group overall had less points. In short, the very act of categorization is enough to trigger intergroup discrimination, and additionally, individuals desire to maximize the difference between the in-group and out-group.\n\n\n\nLinguistic Intergroup Bias\nNaturally, intergroup bias manifests in language. The linguistic intergroup bias (LIB) model, proposed by Anne Maass, describes how systematic biases reflected in language contribute to the perpetuation and persistence of stereotypes (Maass et al. 1989). Specifically, the LIB model highlights differences in levels of abstraction between in-group and out-group linguistics - positive behaviours displayed by in-group members are described in more abstract terms, whereas the same behaviours shown by an out-group member are described in more concrete terms. Inversely, negative behaviours are described more concretely when performed by an in-group member, but in more abstract terms when performed by an out-group member.\nThis differential can be explained by beliefs about the agent’s intrinsic nature, based on their group membership. Describing an observed behaviour abstractly implies that the action is perceived as stable and characteristic of the actor, since such descriptions imply greater generality and probability of future repetition. Concrete descriptions of behaviours, however, suggest that the action is an isolated or uncharacteristic event not necessarily linked to the actor’s enduring qualities. These gaps in levels of abstraction when describing the same action can unsurprisingly result in varying impressions on a hypothetical listener.\nIt is easy to see that such differential language can bolster existing stereotypes - it can be viewed as a self-perpetuating cycle where stereotype-congruent behaviours by the out-group are described in abstract terms that reinforce the prior (negative) expectations and perceptions of the out-group. Unexpected positive actions by the out-group are described in concrete terms, suggesting that they are uncharacteristic and are isolated events. These tendencies keep the unfavorable perception of the out-group intact. One can imagine especially how such differential language can have longstanding and consequential impacts in particular given the extensive use of social media in the modern age, which allows for the rapid dissemination of language in large volumes.\nAccording to Maass et al. (1989), two main explanations for the underlying mechanism for the LIB are 1) in-group protection, and 2) differential expectancy.\nThe former is motivated and based loosely on Tajfel’s Social Identity Theory (SIT) - it hypothesises that the LIB serves to protect or enhance one’s social identity. A key element of the SIT is positive distinctiveness, i.e., the desire for members of a group to portray the in-group as not only different, but different in a positive way, from the out-group. The LIB can be considered a potential strategy of maintaining a positive light of one’s in-group.\nThe second explanation suggests that the LIB may be a result of differential expectancy - variations in a speaker’s expectations of the behaviours of the in-group and out-group. Abstract language may be preferred when behaviours confirm prior expectations, while concrete language is preferred when unexpected or surprising behaviours are observed. Hence, because individuals tend to expect more desirable behaviours from their in-group, they naturally utilize abstract language when referring to the positive behaviours, and concrete language when referring to the unexpected negative behaviours.\n\n\nQuantifying Intergroup Bias\nMeasures of psychological valence, i.e., measures of the positivity of words, have been a longtime tool in the assessment of intergroup bias. Schmidtke and Kuperman (2024) used corpus-based lexical estimates of valence to investigate bias in intergroup scenarios. Participants were assigned to either the “in-group” or “out-group” conditions and were given a set of “seed” words. In the “in-group” condition, participants were told that the seed words described their in-group, and were asked to provide three more words to describe their group. Those in the “out-group” condition were told that the seed words described an out-group, and were asked to provide three more words to describe that out-group. The intention of the experiment was to assess differences in valence in the words produced by participants of either group. To control the “ground truth,” the valence of the seed words were manipulated such that they ranged from highly positive to highly negative.\nIt was found, expectedly, that the valence of words directed towards the in-group was higher than that of words directed at the out-group. With regards to the seed “ground truth” valence, it was found that evaluations about the out-group were much closer to the seed valence, while evaluations about the in-group were consistently inflated. Furthermore, the magnitude of inflation from the “ground truth” tended to be larger when the valence of the seed words were more negative, suggesting that there is an implicit tendency to dissociate one’s in-group from negative characterizations.\nAs an extension to their first experiment, Schmidtke and Kuperman (2024) also examined the propagation of intergroup bias throughout successive generations. Using the linear difusion chain paradigm (which feeds responses of one generation to a next generation, who is given the same task, in this case of providing additional words based on seed words and group membership), it was found that there was a gradual increase in positivity in successive in-group generations, and a gradual decline in positivity in successive out-group generations. In short, intergroup bias is amplified across generations. As in the former experiment, the valence of the seed words did impact the magnitude of amplicifcation. For evaluations directed at the in-group, the positive inflation of generation was significant when the seed words were of low and mid valence, but not of high valence. For evaluations of directed at the out-group, the negative effect of generation was significant when the seed words were of mid and high valence, but not of low valence. Examination of the actual slopes of each model indicated that the desire to enhance initially negative descriptions of the in-group are stronger than the desire to derogate negative traits of the out-group.\n\n\n\n\n\nReferences\n\nMaass, Anne, Daniela Salvi, Luciano Arcuri, and Gün R. Semin. 1989. “Language Use in Intergroup Contexts: The Linguistic Intergroup Bias.” Journal of Personality and Social Psychology 57 (6): 981–93. https://doi.org/10.1037/0022-3514.57.6.981.\n\n\nSchmidtke, Daniel, and Victor Kuperman. 2024. “A Psycholinguistic Study of Intergroup Bias and Its Cultural Propagation.” Scientific Reports 14 (1): 8613. https://doi.org/10.1038/s41598-024-58905-y.\n\n\nTajfel, Henri. 1970. “Experiments in Intergroup Discrimination.” Scientific American 223 (5): 96–103. http://www.jstor.org/stable/24927662.\n\n\n———. 1979. “Individuals and Groups in Social Psychology*.” British Journal of Social and Clinical Psychology 18 (2): 183–90. https://doi.org/10.1111/j.2044-8260.1979.tb00324.x.",
    "crumbs": [
      "Topics",
      "Psychology",
      "Linguistic Intergroup Bias"
    ]
  },
  {
    "objectID": "topics/psychology/02_music_preference.html",
    "href": "topics/psychology/02_music_preference.html",
    "title": "Music Preference & Personality",
    "section": "",
    "text": "It intrigues me to think about why we like the music we like - how much does our music preference have to do with our personality?\nThose who know me know that I am a music fanatic. My music taste has evolved dramatically over the past several years - in the past, I enjoyed pop and electronic dance music (EDM), but now, I almost exclusively listen to heavier music, including genres like pop punk, hard rock, and metalcore. In fact, I used to think it was impossible for anyone to actually enjoy songs where the artist is yelling or screaming at the top of their lungs, but such songs are now a focal (and highly enjoyable) part of my music taste (case in point: ARTIFICIAL SUICIDE by Bad Omens, which I listened to 650+ times in the past 6 months alone). I wonder what factors are behind this shift, as well as what this says about how my personality has evolved over the years.",
    "crumbs": [
      "Topics",
      "Psychology",
      "Music Preference & Personality"
    ]
  },
  {
    "objectID": "topics/psychology/02_music_preference.html#introduction-motivation",
    "href": "topics/psychology/02_music_preference.html#introduction-motivation",
    "title": "Music Preference & Personality",
    "section": "",
    "text": "It intrigues me to think about why we like the music we like - how much does our music preference have to do with our personality?\nThose who know me know that I am a music fanatic. My music taste has evolved dramatically over the past several years - in the past, I enjoyed pop and electronic dance music (EDM), but now, I almost exclusively listen to heavier music, including genres like pop punk, hard rock, and metalcore. In fact, I used to think it was impossible for anyone to actually enjoy songs where the artist is yelling or screaming at the top of their lungs, but such songs are now a focal (and highly enjoyable) part of my music taste (case in point: ARTIFICIAL SUICIDE by Bad Omens, which I listened to 650+ times in the past 6 months alone). I wonder what factors are behind this shift, as well as what this says about how my personality has evolved over the years.",
    "crumbs": [
      "Topics",
      "Psychology",
      "Music Preference & Personality"
    ]
  },
  {
    "objectID": "topics/psychology/02_music_preference.html#the-link-between-music-preference-and-personality",
    "href": "topics/psychology/02_music_preference.html#the-link-between-music-preference-and-personality",
    "title": "Music Preference & Personality",
    "section": "The Link between Music Preference and Personality",
    "text": "The Link between Music Preference and Personality\nMany studies have found that there is indeed a connection (to some extent) between one’s music preferences and their personality traits. Several of these studies have yielded findings in support of interactionist theories, wherein people seek social contexts that provide them with self-confirmatory feedback (Swann Jr., Rentfrow, and Guinn 2003). As in, people intentionally select musical environments that reflect their psychological traits and needs.\nAnderson et al. (2021) conducted a study that aimed to gauge whether the Big Five personality traits could be predicted by music preference and music listening habits through machine learning.\n\nThe Big Five Personality Traits\nThe Big Five personality traits are a group of five key dimensions of personality, outlined by the Five Factor Model, that provide a comprehensive framework for understanding and summarizing human personality (Roccas et al. 2002). These traits are generally measured using self-report questionnaires, where respondants rate their agreement with various statements through a Likert scale.\nThe traits as described by John and Srivastava (1999):\n\nOpenness to experience: one’s willingness to try new things, and to engage in imaginative or intellectual activities. Those who score high in openness tend to be creative, intellectually curious, and open to new experiences, while low-scorers generally prefer routine and are uncomfortable with change.\nConscienciousness: one’s desire to take obligations and tasks seriously. This personality trait is defined by high levels of thoughtfulness, responsibility, and goal-oriented behaviours. Those who score high in conscientiousness are usually considerate, self-disciplined, goal-oriented, and have good impulse control, whereas low-scorers struggle to complete tasks due to poor impulse control.\nExtraversion: one’s tendency to seek (usually social) interaction with their environment. This manifests in outgoing, energetic, and talkative behaviours. Those high in extraversion (i.e., are extroverts) are sociable and are energized by social interaction, whereas those low in extraversion (i.e., are introverts) are reserved, reflective, happy in solitude, and may feel fatigued from social interaction.\nAgreeableness: one’s tendency to act in a cooperative, unselfish manner. Highly agreeable people are sensitive to the needs of others, and are described as trustworthy, sympathetic, and considerate. Those low in agreeableness may be perceived as competitive, uncooperative, and even manipulative.\nNeuroticism: describes one’s emotional stability - it takes into account how likely a person is to interpret events as threatening or difficult, as well as one’s propensity for negative emotions. Those scoring high in neuroticism tend to be lower in self-esteem and more anxious and irritable, while those low in neuroticism are calm, secure, resilient, and self-satisfied.\n\n\nThis study involved over 17.6 million streams (662,000 hours of music) listened to by 5,808 users on Spotify over a 3-month period. Information about user demography, musical preferences, and listening habits were collected to use as input to the machine learning model.\nThe input features are described as follows:\n\nGenre and mood vectors: Each users’ listening data was mapped to two vectors: genre and mood vectors. Spotify generates its genre labels internally, using a combination of aggregated listening patterns (clusters of user behavior), audio features, and cultural knowledge. To produce the mood vectors, the researchers used a third-party service called Gracenote, which analyzes the audio signals of each song - features like rhythm and harmony - and uses a classifier to assign a mood label to the track. 66 Spotify genres and 25 Gracenote moods were represented across all study participants. Each users’ streams were mapped to these genres and moods, and then aggregated and normalized to get the percentage of listening from each genre or mood.\n\nEach user ends up with a length 66 genre vector, and a length 25 mood vector, where each element in either vector indicates the percentage of that user’s listening data attributed to each genre or mood. E.g., say my genre vector looked something like [0.63, 0.24, 0.04, 0.09, 0, … 0], where the corresponding genres are [metalcore, pop punk, pop, rap, &lt;everything else&gt;].\n\n\nDerived metrics: Features were constructed using data provided by Spotify that describes user behaviour on the platform. These features included simpler ones such as the user’s type of device (e.g., Mac, game console) and number of playlists, as well as computed features that descibe four aspects of user listening behaviour, namely:\n\nDiscovery: indicates how open a user is to exploring new music. When considering the first time a user listens to a song as when they “discovered” the song, researchers computed users’ historical discovery rate (total number of unique songs / total number of streams) and windowed discovery rate (the percentage of new songs across listening in the last 30 days). However, discovery rate is not consistent - people tend to have periods of discovery interspersed amongst periods of low discovery (i.e., listening to familiar songs). So, researchers also calculated the historical spread of the windowed discovery rate (how much variation there is in a user’s discovery rate over time), as well as the difference between the current and historical rates of discovery.\nDiversity: a quantification of how diverse a user’s music taste is. This was computed by projecting songs into multidimensional space based on co-occurrences in other user-generated playlists. Songs that are often added together in playlists are closer to each other, while songs that don’t often co-occur have more distance between them. If a user listens to songs that are far apart in this space, this indicates that their music taste is more diverse. The researchers measured aggregate diversity (aggregate distance the user has traversed in the space) and sequential diversity (the average distance between one song and the next) over daily and monthly windows. Standard deviation of the daily windows was also measured to indicate how each user’s diversity changes from day to day.\nContextual listening and audio features: determines what a user’s typical audio fingerprint is, and whether it changes based on context. For example, someone may listen to jazz on weekdays and metal on weekends. A measure called the Shannon Entropy was computed to descibe how each user distributes their listening across different contents (genre, mood, or audio attributes) or contexts (time of day or week).\nNostalgia: denotes a user’s tendency to listen to music representative of their formative years. A metric was computed to describe for each user how much they listen to music that is characteristic of and relevant to their generation.\n\nProduct flag: Spotify has a free version and a Premium version, where the free tier is available without charge but comes with restrictions such as no on-demand streaming and limited numbers of skips. The Premium version gives users unlimited access to their music. These differences impact how the user engages with Spotify, so this feature indicates whether each user has Spotify for free or Spotify Premium.\n\nThe target variables for this study were the Big Five personality Traits, which were measured for each participant via the Big Five Inventory, a 44-item (Likert-style) questionnaire that measures the five traits.\nThe goal of the machine learning model was to independently predict numerical values of each of the Big Five personality traits, given a user’s features. Model selection was done with consideration of the concern of overfitting caused by the large number of features (there were 211, including all genre, mood, demographic, and derived features). Using regularized methods is a common means of addressing overfitting - it does so by penalizing the model coefficients that cause the overfitting (more technically, by adding a penalty term to the loss function, which ultimately punishes overly complex models).\n\nLasso and Ridge Regression\n\nL1 or lasso regression adds the sum of the absolute values of the coefficients to the cost function, which can shrink some coefficients to exactly zero, essentially excluding less important features (feature selection).\nL2 or ridge regression adds the sum of the square of each coefficient to the cost function, which encourages the model to keep coefficients small while not completely eliminating them. Ridge regularization is especially useful when features are correlated, because it distributes their influence across the features.\n\n\nResearchers in this study chose elastic net regularization regression, which combines the strengths of lasso and ridge regression to address datasets with large numbers of features while also outperforming lasso and ridge regression individually (Zou and Hastie 2005). Additionally, random forest regression was also used to account for potentially nonlinear relationships between predictors, such as differences in personality traits amongst Spotify Free and Premium users.\nFor each model, 10-fold cross-validation in which all variable selection and parameter tuning was done within each training set independently, was used to ensure robust results. Prediction accuracy was measured by computing the Pearson correlation coefficient between the predicted values (for each personality trait) and the actual measured values for the test group of each fold. The final reported results are the average correlations across each fold.\n\nThe results that each of the Big Five traits are predicted by musical preferences and habitual listening behaviours with moderate to high accuracy. Conscienciousness and emotional stabilty (opposite of neuroticism) were found to be the two most predictable traits. The Pearson correlation coefficients are shown in the below table.\n\nThere were several interesting correlations between personality traits and certain genres or moods:\n\nOpenness: positively correlated with Atmospheric, Folk, Reggae, or Afropop, which are less popular genres, as well as “sentimental” or “melancholy” music\nEmotional stability: positively correlated with Blues, Old Country, Soul, and “lively” music, and negatively correlated with Indie, Emo, and Regional Music from Korea, as well as with “brooding” (e.g., Take Care by Drake; Karma Police by Radiohead) or “Defiant” music (e.g., Mask Off by Future and 3005 by Childish Gambino).\nAgreeableness: positively correlated with Jazz, Soul, and “sophisticated” music (e.g., Fly Me to the Moon by Frank Sinatra), and negatively correlated with Punk, Death metal, or other “aggressive” music.\nConscientiousness: positively correlated with Funk, Easy Listening, or “Romantic” music (e.g., La Vie en Rose by Edith Piaf), and negatively correlated with Rock, Comedy, and Alternative genres, as well as with “Energizing” (e.g., Happy by Pharrell Williams) and “Excited” (e.g., California Gurls by Katy Perry) moods.\nExtraversion: positively correlated with Funk, Reggaeton, or “sensual” music (e.g., Same Old Love by Selena Gomez), and negatively correlated with Rock, Metal, and “urgent” music (e.g., Locked Out of Heaven by Bruno Mars; Cheap Thrills by Sia)\n\nThere were also correlations between the derived metrics and personality traits:\n\nEmotional stability correlated negatively with average skip rate, indicating that those who were more neurotic tended to be more selective of what they listened to at any given moment.\nOpenness correlated positively with all-time track discovery rate and genre entropy, indicating predictably that high scorers in Openness are more willing to explore new music and different types of music.\nThose scoring high in Conscienciousness tended to concentrate their listening to a narrow window of time in the day across multiple weeks, indicating they structure their day more rigidly than lower scorers.\nThose high in Extraversion tended to listen more from others’ playlists, which can have many possible explanations. For example, they may be more reliant on their social networks’ music suggestions, or they may prefer to discover new music based on their friend group.\nThose who listened to more Spotify-suggested music tended to score higher in Agreeableness or Conscientiousness.",
    "crumbs": [
      "Topics",
      "Psychology",
      "Music Preference & Personality"
    ]
  },
  {
    "objectID": "topics/psychology/02_music_preference.html#conclusion-insights-self-study",
    "href": "topics/psychology/02_music_preference.html#conclusion-insights-self-study",
    "title": "Music Preference & Personality",
    "section": "Conclusion & Insights (self-study)",
    "text": "Conclusion & Insights (self-study)\nI decided to take the Big Five personality test so I could see if the paper’s findings generally align with what I know about myself. I took the online test from Truity (link here) and another from bigfive-web (link here) for cross-validation purposes. The former was 50 questions, while the latter was 120 (questions were more detailed).\nHere are my results from Truity (100-scale):\n\nAnd here are my results from bigfive-web (120-scale):\n\nIn summary from bigfive-web, I score high in all categories except for Extraversion. This is in agreement with the Truity scores, though perhaps a bit less extreme. I’m more willing to trust the bigfive-web results, since the questions were more detailed and I do think I put more thought into them. However, the questionnaire used in the study had less than 50 questions, so in terms of reliability and consistency, I’ll still keep the Truity scores in mind.\nbigfive-web also gives detailed breakdowns of individual aspects of each personality trait and how I score in them. You can review that by downloading the PDF here.\nHere’s how I align (or don’t align) with the study findings. Quick note that the study uses Emotional Stability instead of Neuroticism, and high Neuroticism (me) = low Emotional Stability:\n\nOpenness (high):\n\nPositive correlation with genre entropy (aligns): I think I’d agree with this. Although I listen mostly to metalcore, rock, and pop punk, I mix in some rap / hip-hop and pop. I like the occasional Joyner Lucas and Pop Smoke to hype me up before big events and also the classic pop “white girl bangers” (Katy Perry, Kesha) in the car on the way to the gym. Jazz shows up at times as well when I’m practicing saxophone or just chilling.\nPositive correlation with # of playlists made by user (aligns): I have over 140 playlists - I think that’s a lot. They denote different genres, artists, moods, or feelings for me, and I love making them.\nPositive correlation with all time # of unique tracks and artists (aligns): I imagine these numbers are pretty high, but I’m not sure what is considered “high” in the study. I’ve streamed over 1.5K unique songs from 400-500 unique artists each year from 2022 to 2024 (numbers from lastfm).\nPositive correlation with “melancholy,” “somber,” “peaceful” moods (does not align): I don’t think I listen to a lot songs with these moods. “Peaceful” maybe, due to my study music (Novo Amor, Portair).\n\nConscienciousness (high):\n\nPositive correlations with Country, Blues, Soul, and Funk genres, and negative correlations with Rock, Alternative, and any “aggressive” music (does not align): I appear to be inversely correlated to those trends.\n\nAgreeableness (high):\n\nPositive correlations with Jazz, Soul, Funk genres and “romantic” and “sentimental” moods, and negative correlations with Punk, Emo, Metal, Alternative, and Rock genres, as well as any “aggressive” music (does not align): same as with Conscienciousness - I clearly don’t align much here.\n\nEmotional stability (low):\n\nNegative correlations with Emo, Punk, Modern Rock, Indie, Alternative, and Rock genres (aligns): yeah this describes my music taste pretty well.\n\nExtraversion (low):\n\nPositive correlation with % of mobile listening from others’ playlists (aligns): I pretty much only listen to my own playlists.\nNegative correlation with Punk, Rock, Gothic, Emo, Death Metal, Metal, Alternative, Modern Rock, and “aggressive” music (aligns): funny how introverted people like loud music. This aligns very well with my taste.\n\n\nTo conclude, there are lots of fascinating trends and patterns revealed by Anderson et al. (2021). While I resonate with several, there are obviously many where I don’t align, which makes sense, because human behaviour is difficult to generalize. I do find it very interesting how louder, aggressive music (much of my current music taste) can predict higher Neuroticism and lower Extraversion - I wonder why that is, and it may be worthy of a future deep dive. There are also lots of other factors not directly related to personality that can influence music preference, such as culture, age, and situational factors, that can be further explored.\nTo answer my intial question of whether changes in my personality helped drive my changes in music taste, I’m not sure I can draw any sort of conclusions on that. I’ve absolutely become more open-minded over the years, which is what drew me to listen to new music in the first place, hence helping me realize I really like metalcore and aggressive music. I believe I’ve become more emotionally secure and extraverted over time as well, which, based on the study, might push me away from louder music. However, introvertedness and neuroticism does run in the family, and it’s likely not something I can change in myself for good, so perhaps I always would have liked the music I like now, if I’d been open enough back then to discover it.",
    "crumbs": [
      "Topics",
      "Psychology",
      "Music Preference & Personality"
    ]
  },
  {
    "objectID": "topics/psychology/02_music_preference.html#my-music-recommendations",
    "href": "topics/psychology/02_music_preference.html#my-music-recommendations",
    "title": "Music Preference & Personality",
    "section": "My Music Recommendations",
    "text": "My Music Recommendations\nArtists:\n\nWaterparks (pop punk, pop rock)\nBad Omens (metalcore, alternative metal)\nSleep Token (progressive metal, alternative metal)\nMotionless in White (metalcore, gothic metal)\nDominic Fike (alternative hip-hop, rap rock, alternative rock, indie pop/rock)\n\nMy HARD & HEAVY playlist:",
    "crumbs": [
      "Topics",
      "Psychology",
      "Music Preference & Personality"
    ]
  },
  {
    "objectID": "topics/notes/02_speed_dating_questions.html#fun",
    "href": "topics/notes/02_speed_dating_questions.html#fun",
    "title": "Speed Dating Questions to Build Connections",
    "section": "Fun",
    "text": "Fun\n\nWould you rather be able to communicate with all animals, or speak all foreign languages?",
    "crumbs": [
      "Topics",
      "Notes",
      "Speed Dating Questions to Build Connections"
    ]
  },
  {
    "objectID": "topics/notes/02_speed_dating_questions.html#self",
    "href": "topics/notes/02_speed_dating_questions.html#self",
    "title": "Speed Dating Questions to Build Connections",
    "section": "Self",
    "text": "Self\n\nWhat is something you’re looking forward to in the next few weeks / months?\nWhat is a project or goal that you’re currently working on that excites you?\nHow do you manage stress?\nWhat is your favorite time of the day?",
    "crumbs": [
      "Topics",
      "Notes",
      "Speed Dating Questions to Build Connections"
    ]
  },
  {
    "objectID": "topics/notes/02_speed_dating_questions.html#background",
    "href": "topics/notes/02_speed_dating_questions.html#background",
    "title": "Speed Dating Questions to Build Connections",
    "section": "Background",
    "text": "Background\n\nWhere did you grow up?\nDo you have any siblings? Tell me about them.\nWhat was high school like?\nHow did your childhood experiences shape who you are today?\nWhat is your favorite childhood memory?\nDo you have any family members you particularly admire? Why?\nHow did your family influence your educational or career choices?\nWho had the biggest impact on your life growing up, and why?\nWhat are your friends like?",
    "crumbs": [
      "Topics",
      "Notes",
      "Speed Dating Questions to Build Connections"
    ]
  },
  {
    "objectID": "topics/notes/02_speed_dating_questions.html#hobbies-and-interests",
    "href": "topics/notes/02_speed_dating_questions.html#hobbies-and-interests",
    "title": "Speed Dating Questions to Build Connections",
    "section": "Hobbies and Interests",
    "text": "Hobbies and Interests\n\nHow do you like to spend your free time?\nWhat is the hobby that you are most passionate about?\nWhat is the most recent hobby that you’ve picked up?\nWhat is a hobby you’ve always wanted to try, but haven’t yet?\nWhat kind of music do you like?\nDo you play a musical instrument? (If no, what musical instrument would you like to learn?)\nWhat is a book you think everyone should read / what is your favorite book?\nDo you have any interests that you think would surprise people?\nWhat is one thing you want to learn or do in the next year?\nWhat is a subject you wish you knew more about?",
    "crumbs": [
      "Topics",
      "Notes",
      "Speed Dating Questions to Build Connections"
    ]
  },
  {
    "objectID": "topics/notes/02_speed_dating_questions.html#relationships",
    "href": "topics/notes/02_speed_dating_questions.html#relationships",
    "title": "Speed Dating Questions to Build Connections",
    "section": "Relationships",
    "text": "Relationships\n\nWhat are you looking for in a relationship right now?\nWhat qualities do you value most in a partner? / What is the number one quality you look for in a partner?\nHow do you define a successful relationship?\nWhat is one lesson you’ve learned from past relationships?\nWhat is a relationship deal-breaker for you?\nWhat do you think is the most annoying thing about dating these days?",
    "crumbs": [
      "Topics",
      "Notes",
      "Speed Dating Questions to Build Connections"
    ]
  },
  {
    "objectID": "topics/notes/02_speed_dating_questions.html#career",
    "href": "topics/notes/02_speed_dating_questions.html#career",
    "title": "Speed Dating Questions to Build Connections",
    "section": "Career",
    "text": "Career\n\nWhat is your dream job?\nWhat are your biggest professional goals for the next few years?\nWhat inspired you to choose your current career path?",
    "crumbs": [
      "Topics",
      "Notes",
      "Speed Dating Questions to Build Connections"
    ]
  },
  {
    "objectID": "topics/notes/02_speed_dating_questions.html#social-media",
    "href": "topics/notes/02_speed_dating_questions.html#social-media",
    "title": "Speed Dating Questions to Build Connections",
    "section": "Social media",
    "text": "Social media\n\nWhat is your most-used social media platform?\nDo you think social media brings people closer together or drives them apart?",
    "crumbs": [
      "Topics",
      "Notes",
      "Speed Dating Questions to Build Connections"
    ]
  },
  {
    "objectID": "topics/notes/02_speed_dating_questions.html#deeper",
    "href": "topics/notes/02_speed_dating_questions.html#deeper",
    "title": "Speed Dating Questions to Build Connections",
    "section": "Deeper",
    "text": "Deeper\n\nWhat does happiness mean to you?\nHow do you measure success in life beyond career achievements?\nWhat is the best piece of advice you’ve ever received?",
    "crumbs": [
      "Topics",
      "Notes",
      "Speed Dating Questions to Build Connections"
    ]
  },
  {
    "objectID": "topics/computer_science/neural_networks/02_graph_neural_networks.html",
    "href": "topics/computer_science/neural_networks/02_graph_neural_networks.html",
    "title": "Machine Learning with Graphs",
    "section": "",
    "text": "Graphs are a general language for describing and analyzing entities with relations or interactions.\nMany types of data can naturally be represented by graphs - e.g., disease pathways, food webs, and underground networks. Similarly, social networks, citation networks, and the internet can also inherently be represented as graphs. The power in graphs is that we can represent knowledge and facts as relationships between different entities - for example, the regulatory mechanisms in our cells can be described as processes governed by the connections between different entities.\n\nConsider two types of domains that can be represented graphically:\n\nNetworks (or Natural Graphs):\n\nsocial networks\ncommunication and transactions\nbiomedicine (interactions between genes and proteins)\nbrain connections\n\nGraphs:\n\ninformation or knowledge\nsoftware\nsimilarity networks\nrelational structures\n\n\n\nThe key question we’d like to address is: how do we take advantage of relational structure in order to make better predictions?\n\nComplex domains often have a rich relational structure, which can be represented as a relational graph. And, by modeling relationships in such a way, we can achieve better predictive performance.\n\nThe difficulty of using graphical data lies in their complexity - they are arbitrarily-sized and have complex topological structures, unlike text or image data. So, we would like to construct neural networks that are generalizable to graphs - neual networks that take a graph as input, and ultimately output predictions.\n\n\nIn particular, we would like to implement representation learning, where embeddings are generated to capture both structural and feature-related information from the graph (in place of feature engineering in classical ML algorithms).\n\nIn representation learning, we map nodes to \\(d\\)-dimensional embeddings such that similar nodes in the network are embedded in close proximity in \\(d\\)-dimensional space.\n\nSo, the aim is to learn the function \\(f\\) that takes each node \\(u\\) and maps it to their \\(d\\)-dimensional embeddings, i.e., \\(f: u \\to \\mathbb{R}^d\\), such that “similar” nodes in the graph are also close to each other in space.",
    "crumbs": [
      "Topics",
      "Computer Science",
      "Neural Networks",
      "Machine Learning with Graphs"
    ]
  },
  {
    "objectID": "topics/computer_science/neural_networks/02_graph_neural_networks.html#why-graphs",
    "href": "topics/computer_science/neural_networks/02_graph_neural_networks.html#why-graphs",
    "title": "Machine Learning with Graphs",
    "section": "",
    "text": "Graphs are a general language for describing and analyzing entities with relations or interactions.\nMany types of data can naturally be represented by graphs - e.g., disease pathways, food webs, and underground networks. Similarly, social networks, citation networks, and the internet can also inherently be represented as graphs. The power in graphs is that we can represent knowledge and facts as relationships between different entities - for example, the regulatory mechanisms in our cells can be described as processes governed by the connections between different entities.\n\nConsider two types of domains that can be represented graphically:\n\nNetworks (or Natural Graphs):\n\nsocial networks\ncommunication and transactions\nbiomedicine (interactions between genes and proteins)\nbrain connections\n\nGraphs:\n\ninformation or knowledge\nsoftware\nsimilarity networks\nrelational structures\n\n\n\nThe key question we’d like to address is: how do we take advantage of relational structure in order to make better predictions?\n\nComplex domains often have a rich relational structure, which can be represented as a relational graph. And, by modeling relationships in such a way, we can achieve better predictive performance.\n\nThe difficulty of using graphical data lies in their complexity - they are arbitrarily-sized and have complex topological structures, unlike text or image data. So, we would like to construct neural networks that are generalizable to graphs - neual networks that take a graph as input, and ultimately output predictions.\n\n\nIn particular, we would like to implement representation learning, where embeddings are generated to capture both structural and feature-related information from the graph (in place of feature engineering in classical ML algorithms).\n\nIn representation learning, we map nodes to \\(d\\)-dimensional embeddings such that similar nodes in the network are embedded in close proximity in \\(d\\)-dimensional space.\n\nSo, the aim is to learn the function \\(f\\) that takes each node \\(u\\) and maps it to their \\(d\\)-dimensional embeddings, i.e., \\(f: u \\to \\mathbb{R}^d\\), such that “similar” nodes in the graph are also close to each other in space.",
    "crumbs": [
      "Topics",
      "Computer Science",
      "Neural Networks",
      "Machine Learning with Graphs"
    ]
  },
  {
    "objectID": "topics/computer_science/neural_networks/02_graph_neural_networks.html#applications-of-graph-ml",
    "href": "topics/computer_science/neural_networks/02_graph_neural_networks.html#applications-of-graph-ml",
    "title": "Machine Learning with Graphs",
    "section": "1.2 - Applications of Graph ML",
    "text": "1.2 - Applications of Graph ML\nRepresenting data as graphs allows us to formulate tasks at different levels:\n\nnode-level\n\nA classic node-level task is node classification - predicting the property of a node, such as categorizing online users or items.\n\nedge-level\n\nA classic edge-level task is link prediction - predicting whether there are missing links between a pair of nodes.\n\ncommunity (subgraph) level\n\nA classic community-level task is clustering or community detection - identifying closely-linked subparts of a graph where nodes are densely-connected; e.g., social circle detection.\n\ngraph-level\n\nA classic graph-level task is graph classification - categorizing different graphs; e.g., representing molecules as graphs, and predicting properties of molecules.\n\n\n\n\nNode-level ML Application Examples\nIn node-level tasks, the aim is, for each node in the graph, to predict its position in space.\n\nProtein folding: given a sequence of amino acids, predict the 3D structure of the underlying protein.\n\nThis problem was addressed by Deepmind’s AlphaFold.\nThe key idea that made AlphaFold possible was to represent the protein as a spatial graph where nodes were amino acids, and edges represented proximity between amino acids. By training a GNN on this data, the folding of a protein was able to be simulated by predicting the final positions of amino acids.\n\n\n\n\n\nEdge-level ML Application Examples\nIn edge-level tasks, the aim is to understand the relationships between different nodes.\n\nRecommender systems: think of such systems as users interacting with items, where “items” is a broad term that can emcompass products, movies, songs, etc. The goal of a recommender system is to recommend items that a specific user might like, based on the structure of the graph, as well as the properties of the users and the items.\n\nNodes: users and items\nEdges: user-item interactions The key insight is that we can learn how to embed or represent nodes such that related nodes are embedded closer to each other than nodes that are not related.\n\n\n\n\nPredicting drug side effects: many patients take multiple drugs simultaneously to treat complex or co-existing diseases. The interactions between these many drugs may cause adverse side effects. As such, the goal of this problem is, given an arbitrary pair of drugs, to predict how they interact and cause side effects.\n\nNodes: drugs and proteins\nEdges: interactions\n\n\n\n\n\nSubgraph-level ML Application Examples\n\nTraffic prediction: predicting travel time from one location to another based on traffic conditions on the road segments between those destinations.\n\nNodes: road segments\nEdges: connectivity between road segments\n\n\n\n\n\nGraph-level ML Application Examples\n\nDrug discovery: representing antibiotics (molecules) as small molecular graphs, with the aim of predicting which molecules have therapeutic effects (molecule prioritization for testing in the lab).\n\nNodes: atoms\nEdges: chemical bonds\n\n\n\n\nGenerating novel molecules via graph generation: generating molecules as graphs in a targeted way, and optimizing existing moelcules to have desirable properties.\n\n\n\nPhysical simulation: representing different materials as a set of particles, and use a graph to capture how to particles interact with each other, in order to ultimately predict how the materal deforms.",
    "crumbs": [
      "Topics",
      "Computer Science",
      "Neural Networks",
      "Machine Learning with Graphs"
    ]
  },
  {
    "objectID": "topics/computer_science/neural_networks/02_graph_neural_networks.html#choice-of-graph-representation",
    "href": "topics/computer_science/neural_networks/02_graph_neural_networks.html#choice-of-graph-representation",
    "title": "Machine Learning with Graphs",
    "section": "1.3 - Choice of Graph Representation",
    "text": "1.3 - Choice of Graph Representation\nGraphs (or networks) are composed of:\n\nobjects: nodes or vertices, denoted \\(N\\) or \\(V\\)\ninteractions: links or edges, denoted \\(E\\)\nthe entire system: the network or graph itself - a set of nodes and edges, denoted \\(G(N, E)\\)\n\n\nGraph are versatile - we can represent connections between, say, actors, friends, proteins, and more. As the underlying structure of these graphs are essentially the same, the same ML algorithms can be used to analyze them.\n\nBut, it is important to choose the correct network - the choice of nodes and edges is crucial, as it ultimately determines the nature of the question we are able to study.\n\n \n\nDirected vs Undirected Graphs\nUndirected graphs have undirected links, meaning they are useful in modeling symmetric or reciprocal relationships.\nDirected relationships are captured by directed links, where every link has a direction, source, and destination. * e.g., financial transactions\n\n\n\nNode Degrees\nGiven that we have an undirected graph, we can explore the notion of node degrees, which denotes the number of edges adjacent to a given node.\n\nIt follows that the average node degree is given by the following, which can be simplified to be twice the number of edges in the network divided by the number of nodes.\n\\(\\bar k = \\frac 1 n \\sum^N_{i=1} k_i = \\frac {2E} N\\)\n\nIn directed networks, we define an in-degree and an out-degree, where the in-degree is the number of edges pointing towards the node, while the out-degree is the number of edges pointing outwards from the node.\n\n\n\nBipartite Graphs\nBipartite graphs are a popular type of graph structure - they are graphs whose nodes can be divided into two disjoint sets \\(U\\) and \\(V\\) such that every edge connects a node in \\(U\\) to one in \\(V\\) - i.e., \\(U\\) and \\(V\\) are independent sets.\n\nEvery node only interacts with the other type of node, but not the other.\n\nE.g., authors-to-papers, actors-to-movies, recipes-to-ingredients, customers-to-products\n\n\nWe can also define the concept of folded or projected networks. The idea of folded networks is that, if we have a bipartite graph, we can then project it to either side (using only the nodes from one side) to create a projection graph where nodes are connected if they have at least one neighbour in common.\n\ne.g., if we have a bipartite graph that connects authors to scientific papers, we can create a folded network that becomes a co-authorship network, i.e., authors (nodes) are connected if they co-authored at least one paper in common.\n\n\n\n\nRepresenting Graphs\n\nAdjacency Matrix\nOne way to represent a graph is via an adjacency matrix, which is a square binary matrix \\(A\\) whose elements indicate whether pairs of nodes are adjacent in the graph.\n\n\\(A_{ij} = 1\\) if there is a link from node \\(i\\) to node \\(j\\); \\(A_{ij} = 0\\) otherwise.\nNote that adjacency matrices of undirected graphs are inherently symmetric, as node \\(i\\) being connected to node \\(j\\) automatically means the inverse is also true. However, adjacency matrices of directed graphs are not symmetric.\n\nThe representation of adjacency matrices means that we can compute node degrees by simply summing across a given row or column of the matrix.\n\nFor undirected graphs, the row and column sum for a given node is the same.\nFor nodes of directed graphs, in-degrees are computed by the sum of the columns, while out-degrees are given by the sum of the rows.\n\nIn real-world networks, adjacency matrices are very sparse, which has consequences for the properties of the matrices.\n\ne.g., in a social network that connects people to their friends, the maximum degree that a given node can have is every other human - but it is of course impossible to have 7 billion friends, and so the adjacency matrix of such a problem would be very sparse, since every person’s number of friends is on the order of tens or hundreds and not billions.\n\n\n\n\nEdge List\nAnother way to represent graphs is to use an edge list - a list of pairs of edges. This representation is popular in deep learning frameworks, because it allows us to represent graphs as a 2-dimensional matrix.\n\nHowever, the difficulty of such a representation is that it becomes hard to do graph manipulation or any sort of analysis in the graph - even computing the degree of a node becomes non-trivial.\n\n\n\n\nAdjacency List\nIf graph manipulation and analysis is needed, a preferred graph representation to an edge list is an adjacency list, which associates each node with a list of its neighbours.\n\nThese are easier to work with for large and sparse networks.\n\n\n\n\n\nNode and Edge Attributes\nNodes and edges (entire graphs as well) can be associated with attributes or properties. Some examples:\n\nweights (as an indication of frequency)\nrankings (best friend, second best friend)\ntype (friend, relative, co-worker)\n\nSome properties can be directly represented in adjacency matrices. For example, if edges are weighted, the corresponding value in the adjacency matrix is simply multiplied by that weight.\n\n\n\nMore Types of Graphs\nSome graphs may have self-edges or self-loops, where nodes are connected to themselves. On the adjacency matrix, self-loops are represented by entries on the diagonal of the matrix.\nSome graphs are known as multigraphs, where multiple edges are allowed between a pair of nodes. These graphs can be thought of as a weighted graph, where the entries in the adjacency matrix are non-binary, but it is oftentimes useful to represent the edges individually, as they may have varying properties.\n\n\n\nConnectivity\n\nUndirected Graphs\nA graph is connected if any pair of nodes on the graph can be joined by a path. A disconnected graph is made up by two or more connected components, and may have isolated nodes.\nThere is a useful distinction in adjacency matrices of connected and disconnected graphs. When a graph is disconnected (i.e., has several components), its adjacency matrix can be written in a block-diagonal form, where its nonzero elements are confined in a square area, and all other elements are zero (i.e., there is no connectivity between the components).\n\n\n\nDirected Graphs\nThe notion of connectivity also generalizes to connected graphs - here, we talk about strong vs weak connectivity.\n\nA weakly connected directed graph is a graph that is connected if the directions of the edges are ignored, i.e., each node is connected to another node in some way.\nA strongly connected directed graph is a graph where there exists a directed path between every pair of nodes; i.e., there is a path from node A to node B, as well as from node B to node A.\n\nWe can also speak of strongly-connected components (SCCs), which are sets of nodes in a graph such that every node in the set can visit another node via a directed path.",
    "crumbs": [
      "Topics",
      "Computer Science",
      "Neural Networks",
      "Machine Learning with Graphs"
    ]
  },
  {
    "objectID": "topics/computer_science/neural_networks/02_graph_neural_networks.html#traditional-feature-based-methods-of-ml",
    "href": "topics/computer_science/neural_networks/02_graph_neural_networks.html#traditional-feature-based-methods-of-ml",
    "title": "Machine Learning with Graphs",
    "section": "Traditional Feature-based Methods of ML",
    "text": "Traditional Feature-based Methods of ML\nThe traditional ML pipeline centers around designing the correct features. In particular, we can think about 1) node attributes and 2) structural features that describe how a particular node is positioned within the network.\nUsing effective features is the key to achieving good test performance for node-level, edge-level, and graph-level predictions.\nGiven that our goal is to make predictions for a set of objects, we must make the following design choices:\n\nfeatures: \\(d\\)-dimensional vectors\nobjects: nodes, edges, sets of nodes, entire graphs\nobjective function: what task are we aiming to solve?\n\nMore explicitly, our task is: given a graph \\(G = (V, E)\\), we would like to learn a function \\(f: V \\to \\mathbb{R}\\) that makes predictions. The question becomes how we learn this function \\(f\\).\n \n\n2.1 - Node-Level Tasks and Features\nA common node-level task is node classification, where we would like to assign a category or label to each unlabeled node in a graph.\n\nIn the below example, green nodes have at least two edges, while red nodes have just one.\n\n\nAs seen in the above example, the category of a node is often dependent on how it lies in a network, and as such, we need features that characterize the structure and position of the node in its network and describe its topological pattern. Four approaches to this:\n\nNode degree\n\nThe node degree \\(k_v\\) of node \\(v\\) is defined as the number of edges, or neighbouring nodes, the node has.\nA key limitation of this is that it treats all neighbouring nodes equally without capturing their importance.\n\n\nNode centrality\n\nA more generalized version of node degree is node centrality, denoted \\(c_v\\), which takes the node importance in a graph into account.\n\n\n\nThere are many ways to model node importance:\n\nEigenvector centrality: a node \\(v\\) is important if it is surrounded by important neighbouring nodes \\(u \\in N(v)\\).\n\nThe importance of a given node is given by \\(c_v = \\frac 1 \\lambda \\sum_{u \\in N(v)} c_u\\), where \\(\\lambda\\) is some positive constant. This can also be expressed as an eigenvector problem \\(\\lambda c = Ac\\), where \\(A\\) is the adjacency matrix where \\(A_{uv} = 1\\) if \\(u \\in N(v)\\), and \\(c\\) is the centrality vector. This comes down to determining the eigenvector \\(c\\) associated with the eigenvalue (given) \\(\\lambda\\). People tend to take the leading eigenvector \\(c_{max}\\) that is associated with the largest eigenvalue \\(\\lambda_{max}\\) (which is always positive and unique by Perron-Frobenius Theorem) as a measure of centrality for nodes.\n“The more important my friends are, the higher my own importance is.”\n\n\nBetweenness centrality: a node is important if it lies on many shortest paths between other nodes; a node is important if it is an important bridge between other nodes.\n\nThe importance of a given node is given by \\(c_v = \\sum_{s \\neq v \\neq t} \\frac {\\textnormal{num. shortest paths between s and t that contain v}} {\\textnormal{num. shortest paths between s and t}}\\)\nMeasures how good a connector or transit point a given node is\n\n\nCloseness centrality: a node is important if it has small shortest path lengths to all other nodes in the network.\n\nThe more central you are, the shorter the path to everyone else, and hence the more important you are.\nGiven by \\(c_v = \\frac 1 {\\sum_{u \\neq v} \\textnormal{shortest path length between u and v}}\\) - the closer to the center the node is, the smaller the sum in the denominator will be, which results in a larger measure of importance.\n\n\n\n\nClustering coefficient\n\nThe clustering coefficient measures how connected a given nodes’ neighbours are, and is given by \\(e_v = \\frac {\\textnormal{num. edges among neighbouring nodes of v}} {k_v \\choose 2} \\in [0, 1]\\), where \\(k_v \\choose 2\\) represents the number of node pairs amongst \\(k_v\\) (the degree) neighbouring nodes. This metric measures how actual edges there are out of how many potential edges there could be, and hence it is between 0 and 1.\ne.g., a value of 0 could mean that none of your friends know each other, while a value of 1 means that all of your friends know each other.\n\n\nGraphlets\n\nWe can generalize the notion of the clustering coefficient to that of graphlets, based on the observation that the clustering coefficient is essentially counting the number of triangles in the ego-network of a node.\n\nThe ego-network of a node is a network induced by the node itself and its neighbours (the degree-1 neighbourhood around a given node).\n\nGraphlets are rooted connected non-isomorphic subgraphs (they count shapes beyong triangles) on a larger graph.\n\nWe can define a graphlet degree vector (GDV), which counts the number of graphlets that a node touches. This is ultimately a count vector of graphlets rooted at a given node. These help characterize the local neighbourhood structure around the node of interest based on the frequency of the graphlets that the given node participates in.\n\nE.g., if we were to consider graphlets on 2-5 nodes, we would get a vector of 73 coordinates that describes the topology of a node’s neighbourhood, capturing its interconnectivities out to a distance of 4 hops.\nGDVs provide a measure of a node’s local network topology; comparing vectors of two nodes provides a more detailed measure of local topological similarity than if we were to just look at node degrees or clustering coefficient.\n\n \n\n\n\nImportance-based features, such as node degree and node centrality, help capture the importance of a node in a graph, which becomes useful in predicting influential nodes in a graph (e.g., predicting celebrity users in a social network).\nStructure-based features, such as node degree, clustering coefficient, and graphlet degree vectors, help capture the topological properties of a local neighbourhood around a node. This is useful in predicting the particular role that a given node plays in a graph (e.g., predicting protein functionality in a protein-protein interaction network).\n\n\n\n2.2 - Link Prediction and Link-level Features\nThe task of link prediction is to predict new links based on existing links in the network, meaning that at test time, all node pairs who are not already linked are ranked, and the top \\(K\\) node pairs are predicted. So, the key is to design features for a pair of nodes.\n\nThe link prediction task can be approached in two ways:\n\nRemoving random links from the network\n\nA random set of links are removed from the network, and we aim to predict them\nThis is useful for networks that are static\n\nPredict links over time\n\nIf we have a network that inherently evolves over time, we look at the graph between two times, and based on the edges and the structure of the graph between this time, we output a ranked list \\(L\\) of links that are predicted to appear in a future time.\nThis is useful for transaction or social networks that change over time\n\n\nIn link prediction, we ultimately want to compute a score \\(c(x,y)\\) for each pair of nodes \\((x,y)\\), where \\(c(x,y)\\) could be something like the number of common neighbours of \\(x\\) and \\(y\\), and then to sort the pairs of nodes by decreasing \\(c(x,y)\\) score. * The top \\(n\\) pairs are predicted as the new pairs who will appear in the network.\n\nThe aim in generating link-level features is to describe the relationship between two nodes, such that this relationship allows us to predict whether there exists a link between them or not. Link-level features can be produced in several ways.\n\nDistance-based features\n\nThe shortest-path distance between two nodes - although this does not capture the degree of neighbourhood overlap\n\n\nLocal neighbourhood overlap\n\nCaptures the number of neighbouring nodes shared between two nodes \\(v_1\\) and \\(v_2\\). Many different metrics can be applied:\n\nCommon neighbours: the number of common neighbours shared by the two nodes, computed via \\(|N(v_1) \\cap N(v_2)|\\). Nodes with higher degrees are more likely to have neighbours with each other, and so normalizing is useful.\nJaccard’s coefficient: a normalized version of common neighbours - the size of the intersection divided by the size of the union: \\(\\frac {|N(v_1) \\cap N(v_2)|} {|N(v_1) \\cup N(v_2)|}\\)\nAdamic-Adar index: the sum of the inverse logarithmic degree - the basis of this is that if a node has a lot of neighbours, then the importance of those neighbours is less than if they had few neighbours. This is derived via \\(\\sum_{u \\in N(v_1) \\cap N(v_2)} \\frac 1 {\\textnormal{log}(k_u)}\\).\n\nAnalogy: having a bunch of lesser-connected friends is better than having a bunch of friends with tons of connections to other people (in terms of the importance those friends place on you)\n\n\nThe limitation of local neighbourhood features, though, is that the metric will always return zero if two nodes have no neighbours in common, despite the possibility that those nodes may still be connected in the future - if the nodes are more than two hops apart, their score will be zero.\n\n\nGlobal neighbourhood overlap\n\nExpands upon local neighbourhood features by considering the entire graph.\n\n\n\nKatz index: counts the number of paths of all lengths between a given pair of nodes\n\nWe can use a graph adjacency matrix to compute the number of paths between two nodes. The problem of finding the number of paths reduces down to taking powers of the adjacency matrix - recall that if nodes \\(u\\) and \\(v\\) are connected, then \\(A_{uv} = 1\\). If we let \\(P_{uv}^{(K)}\\) denote the number of paths of length \\(K\\) between nodes \\(u\\) and \\(v\\), then it can be shown that \\(P_{uv}^{(K)} = A^K\\). If we wanted to compute \\(P_{uv}^{(2)}\\), i.e., the number of paths of length 2 between \\(u\\) and \\(v\\), we decompose the problem into two steps:\n\nCompute the number of paths of length 1 between \\(u\\)’s neighbours and \\(v\\)\nFind the sum of those numbers of paths across \\(u\\)’s neighbours\n\nThe Katz index sums over all path lengths for a given set of nodes, and is computed via \\(S_{v_1, v_2} = \\sum_{l=1}^{\\infty} \\beta^l A^l_{v_1, v_2}\\), where \\(0&lt;\\beta&lt;1\\) is a discount factor that gives lower importance to paths of longer lengths. This can be computed in closed-form as so: \\(S = \\sum^{\\infty}_{i=1} \\beta^iA^i = (I - \\beta A)^{-1} - I\\), as it is simply a geometric series for matrices. The entries of \\(S\\) will give the Katz score for any pair of nodes.\n\n \n\n\n\n\n\n2.3 - Graph-level Features and Graph Kernels\nThe goal here is that we want features that characterize the structure of an entire graph. We can do this by using kernel methods, which are widely used in traditional machine learning.\nIn kernel methods, the idea is to design kernels instead of feature vectors, where a kernel \\(K(G, G')\\) between two graphs measures the similarity between those two graphs (their data points). A kernel matrix \\(K = (K(G, G'))_{G,G'}\\), then, measures the similarity between all pairs of data points or graphs. * In order for a kernel to be valid, the kernel matrix must be positive semidefinite, meaning it has positive eigenvalues and is symmetric. * There exists a feature representation \\(\\Phi(*)\\) such that \\(K(G, G') = \\Phi(G)^T \\Phi(G')\\) - i.e., the the kernel between two graphs is simply the dot product of the feature representations of each individual graph. * Once the kernel is defined, off-the-shelf ML models, such as the kernel SVM, can be used to make predictions.\n\nMany graph kernels - which measure similarity between two graphs - exist. Common examples are the graphlet kernel and the Weisfeiler-Lehman kernel, and other examples proposed in literature are the random-walk kernel, the shortest-path graph kernel, and more.\nThe basis of graph kernels is to design some graph feature vector \\(\\Phi(G)\\). We can apply a bag-of-words (BoW) representation of a graph, where we regard nodes as words. However, note that graphs can have many structures that result in the same BoW feature vectors. So, an enhanced version of this is to use a bag-of node degrees method, in which a graph is represented as a feature vector of its node degress (e.g., [1, 3, 0] means 1 node of degree 1, 3 nodes of degree 2, 0 nodes of degree 3). Both the graphlet kernel and the Weisfeiler-Lehman kernel use this idea of bag-of-* representations of graphs, where * is something more sophisticated than node degrees.\n\n\nGraphlet kernel: representing a graph as the count of the number of different graphlets in a graph.\n\nGraphlets here differ from that of node-level graphlets - here, they do not need to be rooted, and do not need to be connected.\nGiven a graph \\(G\\), and a graphlet list \\(g_k = (g_1, g_2, ..., g_{n_k})\\), we define the graphlet count vector \\(f_G \\in \\mathbb{R}^{n_k}\\) as \\((f_G)_i = \\textnormal{num.} (g_i \\subseteq G)\\) for \\(i = 1,2,...,n_k\\) - the number of instances of a given graphlet that appears in our graph of interest.\nGiven two graphs, the graphlet kernel can simply be computed as the dot product between the graphlet count vectors of each graph, i.e., \\(K(G, G') = f_G^Tf_{G'}\\).\n\nHowever, the two graphs may have different sizes, and computing the kernel as such may skew the value. As such, this can be addressed by normalizing each feature vector - each feature vector can be represented as a vector of the count of individual graphlets divided by the total number of graphlets that appear in the graph: \\(h_G = \\frac {f_G}{\\textnormal{sum}(f_G)}\\), and then the kernel is computed as \\(K(G, G') = h_G^Th_{G'}\\).\n\nA key limitation of the graphlet kernel is that counting graphlets is an expensive task - counting size-\\(k\\) graphlets for a graph with size \\(n\\) by enumeration is \\(O(n^k)\\).\n\n \nWeisfeiler-Lehman kernel: aims to address inefficiencies of the graphlet kernel by using neighbourhood structure to iteratively enrich node vocabulary.\n\nThis is a generalized version of the bag-of node degrees method.\n\n\n\nThe algorithm used to achieve this is known as color refinement.\n\nGiven a graph \\(G\\) with a set of nodes \\(V\\), an initial color \\(c^{(0)} (v)\\) is assigned to each node \\(v\\), and then node colors are iteratively refined by hashing (mapping different inputs to different colors) to create new colors: \\(c^{k+1}(v) = \\textnormal{HASH}({c^{k}(v), {c^{k}(u)}_{u \\in N(v)}})\\). Then, after \\(K\\) steps of color refinement, \\(c^{K}(v)\\) summarizes the structure of the graph within a \\(K\\)-hop neighbourhood.\nAs more iterations are done, the node gathers color information from nodes farther away in the network.\n\nAfter color refinement, the Weisfeiler-Lehman kernel counts the number of ndoes with a given color to produce the ultimate feature vector. Then, the Weisfeiler-Lehman kernel value is found by computing the inner product of the color count vectors.\nThe Weisfeiler-Lehman kernel is popular as it is useful and computationally efficient - its time complexity is linear in the number of edges, as it involves aggregating neighbouring colors.\n\nWhen computing a kernel value, only the colors who appear in the two graphs need to be tracked, and hence the number of colors is at most the total number of nodes.\nCounting colors is also linear with respect to the number of nodes.\nThe total time complexity is linear in the number of edges.",
    "crumbs": [
      "Topics",
      "Computer Science",
      "Neural Networks",
      "Machine Learning with Graphs"
    ]
  },
  {
    "objectID": "ideabank.html",
    "href": "ideabank.html",
    "title": "Ideabank",
    "section": "",
    "text": "For additional ideas, generate a random Wikipedia page…\n\n\nPsychology\n\nIntergroup bias and its implications\n\nMaas et al., 1989 (Language use in intergroup contexts: The linguistic intergroup bias.)\nMaas, 1999 (Linguistic Intergroup Bias: Stereotype Perpetuation Through Language)\nSchmidtke & Kuperman, 2024 (A psycholinguistic study of intergroup bias and its cultural propagation)\n\nBiological contributors to homicide\n\nSajous-Turner et al., 2020 (Aberrant brain gray matter in murderers)\nRaine, 2013 (The anatomy of violence: The biological roots of crime) - this is a book\n\nThe psychology of advertising and consumer behaviour - what psychological tactics are used in marketing and why do they work to influence consumer decisions?\nCultural influences on behaviour and cognition\n\nIndividualism vs Collectivism (Hofstede’s Dimensions)\n\nAttachment styles and their impact on adult romantic relationships\n\nSpecifically, I find avoidant attachment styles to be pretty interesting\n\nWhy do people watch horror movies (when they are intended specifically to evoke fear and negative emotions in viewers)?\n\nAndersen et al., 2021 (Playing With Fear: A Field Study in Recreational Horror)\n\nPsychiatric disorders as dynamical systems\n\nScheffer et al., 2024 (A Dynamical Systems View of Psychiatric Disorders-Theory: A Review)\nTweet by Mitchell B. Slapik\n\n\n\n\nComputer Science\n\nTopic modelling - an overview of what it is, how it works, and how to apply it\n\nBlei, 2012 (Probabilistic Topic Models)\n\nGame Theory\n\nApplication of Game Theory - SHAP values in machine learning\n\nNLP - go through Dan Jurafsky and James H. Martin’s textbook\n\n\n\nLinguistics\n\nOnomastics (the study of the history and origin of proper names, especially personal names) and nicknaming\n\nKennedy & Zamuner, 2006 (Nicknames and the Lexicon of Sports)\nBechar-Israeli, 1995 (From 〈Bonehead〉 to 〈cLoNehEad〉: Nicknames, Play, and Identity on Internet Relay Chat)\n\nThe impact of social media on political speech\n\nEtymology Nerd’s article Algorithms are making political speech more extreme\n\nIdiolects - the speech habits that are unique to specific people based on their own life experiences (education, sociological and geographical upbringing, etc.)\n\nInspired by this short article by The Etymology Nerd: how to catch a gubernatorial candidate, or a domestic terrorist\n\nLinguistic Game Theory\n\nInspired once again by The Etymology Nerd (reel)\nGame Theoretic Linguistics (Applications of Game Theory in Linguistics)\n\n\n\n\nAncient History\n\nCassius Dio and his impact on Roman history\nSeries of (evil) emperors / rulers / dictators and their reigns:\n\nEmperor Qin\nCommodus\nCaligula\nCaracalla\n\nSPQR: A History of Ancient Rome page 527 and on (Epilogue: The First Roman Millenium)\n\nNero\nNebuchadnezzar II of Babylon\nHenry VI of England\n\n\n\n\nAstronomy\n\nOlbers’ Paradox\n\nWhy is the sky dark at night? Via Britannica: “If the universe is endless and uniformly populated with luminous stars, then every line of sight must eventually terminate at the surface of a star. Hence, contrary to observation, this argument implies that the night sky should everywhere be bright, with no dark spaces between the stars.”\n\n\n\n\nMarine Science\n\nSeries of deep sea creatures\n\nGiant Pacific Octopus\nColossal Squid\n\nExtinction of the megalodon - proof that it is truly extinct\n\n\n\nPhilosophy\n\nThe Simulation Argument - are we living in a simulation?\n\nVia Wikipedia: “In 2001, philosopher Nick Bostrom proposed the simulation argument, which suggests that if a civilization becomes capable of creating conscious simulations, it could generate so many simulated beings that a randomly chosen conscious entity would almost certainly be in a simulation. The argument presents a trilemma: either such simulations are not created due to technological limitations or self-destruction; or advanced civilizations choose not to create them; or we are almost certainly living in one. This assumes that consciousness is not uniquely tied to biological brains but can arise from any system that implements the right computational structures and processes.”\n\n\n\n\nPaleontology\n\nTiktaalik and transition fossils\n\nThe evolution from swimming fish to four-legged vertebrates\n\n\n\n\nBioinformatics / Computational Biology\n\nAVADA: toward automated pathogenic variant evidence retrieval directly from the full-text literature\n\nI’m super interested in their approach - I can learn a lot from this.",
    "crumbs": [
      "Ideabank"
    ]
  }
]